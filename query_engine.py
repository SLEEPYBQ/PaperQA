import os
import glob
import re
from concurrent.futures import ProcessPoolExecutor
from openai import OpenAI
from tqdm import tqdm
from config import QUESTION_PATTERNS, QUESTION_IDS

def create_combined_prompt(markdown_content):
    """åˆ›å»ºåˆå¹¶çš„æç¤ºè¯"""
    
    prompt_template = """Please analyze the provided research paper and answer the following questions. For each question, provide a concise answer followed by the relevant source text.

QUESTIONS:
1. Involved Stakeholder: What are the involved stakeholders (e.g., elderly people, caregivers, technical solution providers) in the study? Stakeholders must meet one of the following criteria: 1. Participate in experiments or studies; 2. Not participate directly but expressed opinions or perspectives (e.g., via interviews, focus groups); 3. Play a role in shaping the findings or conclusions of the paper.

2. Sample Size: What is the sample size of the study? For example, if 100 people participated and only 90 consented to data collection, the sample size is 90. For multi-study papers, specify the sample size for each study group.

3. Country: What is the country or region of the participants as explicitly stated in the paper (do not infer from the authors' affiliations)?

4. Age: What age-related information is provided in the study (e.g., age range, mean, or median age)?

5. Gender: What gender-related information is reported in the study?

6. Demographic Background: What demographic background information is reported? (For example, socioeconomic status, educational level, and living context for elderly people or working context for caregivers; also include any additional details such as language proficiency, professional background, or technology literacy if mentioned.)

7. Cognitive And Physical Impairment: What cognitive and physical impairments are described among the elderly participants? If standardized measurement tools were used, report the specific scores and the name of the scale; if qualitative terms (e.g., 'mild', 'severe') were used, report them accordingly.

8. Needs And Expectations: What are the explicitly stated or inferred needs and expectations of users, primarily elderly people and caregivers? This includes both directly expressed needs and user preferences accompanied by explanatory comments during interviews or post-trial reflections.

9. Application Context: What is the envisioned application context for the robot as explicitly mentioned in the paper?

10. Process Of The Care: What information is provided about the duration and stage of the care process? Specify whether the study involved a first encounter, short-term use, or long-term deployment, and include session duration and frequency if available.

11. Methodology: What research methodology was used in the study (e.g., qualitative interviews, quantitative surveys, randomized controlled trials)?

12. Care Type: What type of care is the study focused on?

13. Robot Type: What type of robot is used in the study? (If the paper uses terms like 'human-like' or 'animal-like', use those directly; otherwise, provide a short description of the robot's appearance.)

14. Robot Name: What is the name of the robot used in the study?

15. Design Goal: What design goals were set by the solution provider when designing the robot or its interaction functions?

16. Robot Concern Function: What functionalities of the robot were demonstrated, deployed, or introduced to users during the study?

17. Facilitating Functions: What specific robot functions or features are reported to enhance the user experience (i.e., positive features)? Please provide brief explanations for why these features are considered beneficial.

18. Inhibitory Functions: What specific robot functions or features are reported to hinder the user experience (i.e., negative features)? Please provide brief explanations for why these features are considered detrimental.

19. Stakeholder Facilitating Characteristics: What characteristics of the stakeholders are associated with better robot use, acceptance, or trust? Include brief explanations where available.

20. Stakeholder Inhibitory Characteristics: What characteristics of the stakeholders are associated with reduced robot use, lower acceptance, or lower trust? Include brief explanations where available.

21. Engagement: What evaluation of user engagement in the robot is reported in the study? This may include quantitative measurements (e.g., rating scales) or qualitative descriptions (e.g., 'high engagement', 'low acceptance', 'gradual trust development').

22. Acceptance: What evaluation of user acceptance trust in the robot is reported in the study? This may include quantitative measurements (e.g., rating scales) or qualitative descriptions (e.g., 'high engagement', 'low acceptance', 'gradual trust development').

23. Trust: What evaluation of user trust in the robot is reported in the study? This may include quantitative measurements (e.g., rating scales) or qualitative descriptions (e.g., 'high engagement', 'low acceptance', 'gradual trust development').

24. Key Findings: What are the key findings of the study, as typically summarized in the conclusion or discussion section?

25. Additional Info: What additional information is provided about the study, such as limitations or other relevant details?

26. Testing Context: What is the testing context of the study? (For example, was the test conducted in a lab, care home, hospital, private residence, or another setting?)

RESPONSE FORMAT:
Please respond using markdown headers for each question. Use the exact question IDs as headers, followed by your answer and source. Format exactly like this:

## Involved Stakeholder
[Your concise answer. If information is not available, write 'N/A']

Source: [Quote the relevant text from the paper]

## Sample Size
[Your concise answer. If information is not available, write 'N/A']

Source: [Quote the relevant text from the paper]

... continue for all 26 questions using their respective IDs as headers.

RESEARCH PAPER CONTENT:
{content}"""
    
    return prompt_template.format(content=markdown_content)

def query_document_with_combined_questions(markdown_path, client, model, verbose=False):
    """ä½¿ç”¨åˆå¹¶é—®é¢˜æŸ¥è¯¢å•ä¸ªæ–‡æ¡£"""
    try:
        # è¯»å–Markdownæ–‡ä»¶
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        # åˆ›å»ºåˆå¹¶çš„æç¤ºè¯
        combined_prompt = create_combined_prompt(markdown_content)
        
        if verbose:
            print(f"    å‘é€æŸ¥è¯¢è¯·æ±‚...")
        
        # å‘é€è¯·æ±‚åˆ°OpenAI
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": combined_prompt}
            ],
            temperature=0.1,
            max_tokens=4000
        )
        
        result_text = response.choices[0].message.content
        
        if verbose:
            print(f"    âœ… æŸ¥è¯¢æˆåŠŸ")
        
        return True, result_text
        
    except Exception as e:
        if verbose:
            print(f"    âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        return False, str(e)

def parse_combined_response(response_text):
    """è§£æåˆå¹¶å“åº”ï¼Œæå–å„ä¸ªé—®é¢˜çš„ç­”æ¡ˆ"""
    results = {}
    
    # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºçµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    for display_name, question_id in QUESTION_PATTERNS:
        # é¢„å…ˆå¤„ç†è½¬ä¹‰å­—ç¬¦ä¸²ï¼Œé¿å…f-stringä¸­ä½¿ç”¨åæ–œæ 
        escaped_display_name = re.escape(display_name)
        escaped_question_id = re.escape(question_id)
        flexible_name = escaped_display_name.replace(r"\ ", r"\s+")
        
        patterns_to_try = [
            # æ ‡å‡†æ ¼å¼: ## Display Name
            rf'##\s*{escaped_display_name}\s*\n(.*?)(?=\n\s*##|\Z)',
            # å…¨å°å†™: ## display name  
            rf'##\s*{re.escape(display_name.lower())}\s*\n(.*?)(?=\n\s*##|\Z)',
            # å…¨å¤§å†™: ## DISPLAY NAME
            rf'##\s*{re.escape(display_name.upper())}\s*\n(.*?)(?=\n\s*##|\Z)',
            # ä¸‹åˆ’çº¿æ ¼å¼: ## display_name
            rf'##\s*{escaped_question_id}\s*\n(.*?)(?=\n\s*##|\Z)',
            # æ··åˆæ ¼å¼: å…è®¸é¢å¤–çš„ç©ºæ ¼å’Œæ ‡ç‚¹
            rf'##\s*{flexible_name}\s*[:\s]*\n(.*?)(?=\n\s*##|\Z)',
        ]
        
        matched = False
        for pattern in patterns_to_try:
            match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(1).strip()
                
                # æ›´å®Œæ•´çš„Sourceåˆ†ç¦»é€»è¾‘
                # åŒ¹é…å„ç§å¯èƒ½çš„Sourceæ ¼å¼
                source_patterns = [
                    r'\n\s*Source:\s*(.*?)(?=\n\s*$|\Z)',  # æ ‡å‡†æ ¼å¼: Source: xxx (è¡Œå°¾)
                    r'Source:\s*(.*?)(?=\n|$)',           # è¡Œå†…Source: xxx
                    r'\n\s*source:\s*(.*?)(?=\n\s*$|\Z)', # å°å†™source (è¡Œå°¾)
                    r'source:\s*(.*?)(?=\n|$)',           # è¡Œå†…å°å†™source
                    r'\n\s*SOURCE:\s*(.*?)(?=\n\s*$|\Z)', # å¤§å†™SOURCE (è¡Œå°¾)
                    r'SOURCE:\s*(.*?)(?=\n|$)',           # è¡Œå†…å¤§å†™SOURCE
                ]
                
                source_found = False
                for source_pattern in source_patterns:
                    source_match = re.search(source_pattern, content, re.DOTALL | re.MULTILINE)
                    if source_match:
                        source = source_match.group(1).strip()
                        # ç§»é™¤Sourceéƒ¨åˆ†ï¼Œè·å–çº¯ç­”æ¡ˆ
                        answer = re.sub(source_pattern, '', content, flags=re.DOTALL | re.MULTILINE).strip()
                        # ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”ç­”æ¡ˆå’ŒSource
                        results[question_id] = f"{answer}\nSource: {source}"
                        source_found = True
                        break
                
                if not source_found:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°Sourceï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨å†…å®¹
                    results[question_id] = content
                
                matched = True
                break
        
        if not matched:
            results[question_id] = "[è§£æå¤±è´¥] - æ— æ³•ä»å“åº”ä¸­æå–ç­”æ¡ˆ"
    
    return results

def query_documents_wrapper(args_tuple):
    """å¹¶è¡ŒæŸ¥è¯¢æ–‡æ¡£çš„åŒ…è£…å‡½æ•°"""
    markdown_path, api_key, api_base, model, verbose = args_tuple
    
    try:
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url=api_base
        )
        
        doc_name = os.path.basename(markdown_path)
        
        if verbose:
            print(f"  ğŸ”„ æŸ¥è¯¢æ–‡æ¡£: {doc_name}")
        
        # æŸ¥è¯¢æ–‡æ¡£
        success, response = query_document_with_combined_questions(
            markdown_path, client, model, verbose
        )
        
        if success:
            # è§£æå“åº”
            parsed_results = parse_combined_response(response)
            
            if verbose:
                print(f"  âœ… å®Œæˆ: {doc_name}")
            
            return doc_name, True, parsed_results, None
        else:
            if verbose:
                print(f"  âŒ å¤±è´¥: {doc_name}")
            return doc_name, False, None, response
            
    except Exception as e:
        doc_name = os.path.basename(markdown_path) if markdown_path else "æœªçŸ¥æ–‡æ¡£"
        return doc_name, False, None, str(e)

def query_all_documents(args):
    """æŸ¥è¯¢æ‰€æœ‰Markdownæ–‡æ¡£"""
    
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    markdown_files = glob.glob(os.path.join(args.markdown_folder, "*.md"))
    
    if not markdown_files:
        print(f"âŒ åœ¨ {args.markdown_folder} æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°Markdownæ–‡ä»¶")
        return
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(markdown_files)} ä¸ªMarkdownæ–‡ä»¶")
    
    # APIé…ç½®
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ è¯·æä¾›OpenAI APIå¯†é’¥ (é€šè¿‡--api-keyå‚æ•°æˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡)")
        return
    
    # å‡†å¤‡æŸ¥è¯¢å‚æ•°
    query_args = [
        (md_path, api_key, args.api_base, args.model, args.verbose)
        for md_path in markdown_files
    ]
    
    print(f"ğŸš€ ä½¿ç”¨ {args.max_workers} ä¸ªå·¥ä½œè¿›ç¨‹å¹¶è¡ŒæŸ¥è¯¢...")
    print("-" * 50)
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    failed_queries = []
    
    # å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢
    with ProcessPoolExecutor(max_workers=args.max_workers) as executor:
        for doc_name, success, results, error in tqdm(
            executor.map(query_documents_wrapper, query_args),
            total=len(query_args),
            desc="æŸ¥è¯¢è¿›åº¦"
        ):
            if success:
                # æ„å»ºç»“æœå­—å…¸
                doc_result = {"document": doc_name}
                doc_result.update(results)
                all_results[doc_name] = doc_result
            else:
                print(f"âŒ {doc_name}: {error}")
                failed_queries.append((doc_name, error))
                # ä¸ºå¤±è´¥çš„æŸ¥è¯¢æ·»åŠ å ä½ç¬¦
                doc_result = {"document": doc_name}
                for question_id in QUESTION_IDS:
                    doc_result[question_id] = f"[æŸ¥è¯¢å¤±è´¥] - {error}"
                all_results[doc_name] = doc_result
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ğŸ“Š æŸ¥è¯¢å®Œæˆç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸæŸ¥è¯¢: {len(all_results) - len(failed_queries)} ä¸ªæ–‡æ¡£")
    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {len(failed_queries)} ä¸ªæ–‡æ¡£")
    
    if failed_queries:
        print(f"\nâŒ å¤±è´¥çš„æŸ¥è¯¢è¯¦æƒ…:")
        for doc_name, error in failed_queries:
            print(f"  â€¢ {doc_name}: {error}")
    
    return all_results