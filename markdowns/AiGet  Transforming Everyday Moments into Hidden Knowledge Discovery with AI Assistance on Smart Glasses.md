![](_page_0_Picture_0.jpeg)

.

![](_page_0_Picture_1.jpeg)

![](_page_0_Picture_2.jpeg)

Latest updates: [hps://dl.acm.org/doi/10.1145/3706598.3713953](https://dl.acm.org/doi/10.1145/3706598.3713953)

### RESEARCH-ARTICLE

## AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses

[RUNZE](https://dl.acm.org/doi/10.1145/contrib-99660783563) CAI, National University of [Singapore,](https://dl.acm.org/doi/10.1145/institution-60017161) Singapore City, Singapore NUWAN [JANAKA](https://dl.acm.org/doi/10.1145/contrib-99658676612), National University of [Singapore,](https://dl.acm.org/doi/10.1145/institution-60017161) Singapore City, Singapore [HYEONG](https://dl.acm.org/doi/10.1145/contrib-81484650287) CHEOL KIM, National University of [Singapore,](https://dl.acm.org/doi/10.1145/institution-60017161) Singapore City, Singapore YANG [CHEN](https://dl.acm.org/doi/10.1145/contrib-99661213977), National University of [Singapore,](https://dl.acm.org/doi/10.1145/institution-60017161) Singapore City, Singapore [SHENGDONG](https://dl.acm.org/doi/10.1145/contrib-81416592901) ZHAO, City [University](https://dl.acm.org/doi/10.1145/institution-60013983) of Hong Kong, Hong Kong, Hong Kong YUN [HUANG](https://dl.acm.org/doi/10.1145/contrib-99658635613), University of Illinois [Urbana-Champaign,](https://dl.acm.org/doi/10.1145/institution-60000745) Urbana, IL, United States [View](https://dl.acm.org/doi/10.1145/3706598.3713953) all

Open Access [Support](https://libraries.acm.org/acmopen) provided by: National [University](https://dl.acm.org/doi/10.1145/institution-60017161) of Singapore University of Illinois [Urbana-Champaign](https://dl.acm.org/doi/10.1145/institution-60000745) City [University](https://dl.acm.org/doi/10.1145/institution-60013983) of Hong Kong

![](_page_0_Picture_8.jpeg)

PDF Download 3706598.3713953.pdf 16 January 2026 Total Citations: 5 Total Downloads: 4563

Published: 26 April 2025

[Citation](https://dl.acm.org/action/exportCiteProcCitation?dois=10.1145%2F3706598.3713953&targetFile=custom-bibtex&format=bibtex) in BibTeX format

CHI 2025: CHI [Conference](https://dl.acm.org/conference/chi) on Human [Factors in Computing Systems](https://dl.acm.org/conference/chi) *April 26 - May 1, 2025 Yokohama, Japan*

Conference Sponsors: [SIGCHI](https://dl.acm.org/sig/sigchi)

# AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses

[Runze](https://orcid.org/0000-0003-0974-3751) Cai Synteraction Lab, School of Computing National University of Singapore Singapore, Singapore [runze.cai@u.nus.edu](mailto:runze.cai@u.nus.edu)

Yang [Chen](https://orcid.org/0000-0003-3129-8447)<sup>∗</sup> College of Design and Engineering National University of Singapore Singapore, Singapore [cyang@u.nus.edu](mailto:cyang@u.nus.edu)

[Nuwan](https://orcid.org/0000-0003-2983-6808) Janaka Smart Systems Institute, Synteraction lab National University of Singapore Singapore, Singapore [nuwanj@u.nus.edu](mailto:nuwanj@u.nus.edu)

[Shengdong](https://orcid.org/0000-0001-7971-3107) Zhao† School of Creative Media & Department of Computer Science City University of Hong Kong Hong Kong, China shengdong.zhao@cityu.edu.hk

[David](https://orcid.org/0000-0002-2309-4535) Hsu† School of Computing, Smart Systems Institute National University of Singapore Singapore, Singapore dyhsu@comp.nus.edu.sg

[Hyeongcheol](https://orcid.org/0000-0003-4327-2148) Kim<sup>∗</sup> Synteraction Lab National University of Singapore Singapore, Singapore [hyeongcheol@u.nus.edu](mailto:hyeongcheol@u.nus.edu)

Yun [Huang](https://orcid.org/0000-0003-0399-8032) School of Information Sciences University of Illinois at Urbana-Champaign Champaign, Illinois, USA [yunhuang@illinois.edu](mailto:yunhuang@illinois.edu)

#### Abstract

Unlike the free exploration of childhood, the demands of daily life reduce our motivation to explore our surroundings, leading to missed opportunitiesfor informal learning. Traditional toolsfor knowledge acquisition are reactive, relying on user initiative and limiting their ability to uncover hidden interests. Through formative studies, we introduce AiGet, a proactive AI assistant integrated with AR smart glasses, designed to seamlessly embed informal learning into lowdemand daily activities (e.g., casual walking and shopping). AiGet analyzes real-time user gaze patterns, environmental context, and user profiles, leveraging large language models to deliver personalized, context-aware knowledge with low disruption to primary tasks. In-lab evaluations and real-world testing, including continued use over multiple days, demonstrate AiGet's effectiveness in uncovering overlooked yet surprising interests, enhancing primary task enjoyment, reviving curiosity, and deepening connections with the environment. We further propose design guidelines for AI-assisted informal learning, focused on transforming everyday moments into enriching learning experiences.

This work is licensed under a Creative Commons Attribution 4.0 [International](https://creativecommons.org/licenses/by/4.0) License. CHI '25, Yokohama, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1394-1/25/04 <https://doi.org/10.1145/3706598.3713953>

#### CCS Concepts

• Human-centered computing → Ubiquitous and mobile computing systems and tools; Empirical studies in interaction design.

#### Keywords

HMD, smart glasses, AI, large language model, multimodal information, incidental learning, informal learning, wearable-AI assistance, human-ai interaction, knowledge discovery

#### ACM Reference Format:

Runze Cai, Nuwan Janaka, Hyeongcheol Kim, Yang Chen, Shengdong Zhao, Yun Huang, and David Hsu. 2025. AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses. In CHI Conference on Human Factors in Computing Systems (CHI '25), April 26–May 01, 2025, Yokohama, Japan. ACM, New York, NY, USA, [26](#page-26-0) pages. <https://doi.org/10.1145/3706598.3713953>

#### 1 Introduction

"The real voyage of discovery consists not in seeking new landscapes, but in having new eyes." – Marcel Proust.

Our daily surroundings are filled with rich knowledge, from building designs to local flora and fauna, offering countless opportunities for discovery and informal learning from everyday moments [\[6,](#page-19-0) [13,](#page-19-1) [55\]](#page-20-0). Such informal learning not only allows us to gather useful knowledge about our surroundings but also fosters critical thinking and diverse perspectives, helping us adapt to new situations and inspire innovation [\[32,](#page-20-1) [38,](#page-20-2) [72\]](#page-21-0). Yet, modern adult life often disconnects us from these rich opportunities, making us "blind" to

<sup>∗</sup> Both authors contributed equally to this research. †Corresponding Authors.

#### CHI '25, April 26–May 01, 2025, Yokohama, Japan Cai et al.

<span id="page-2-0"></span>![](_page_2_Picture_2.jpeg)

Figure 1: A day with AiGet, a wearable knowledge discovery assistant equipped with AR smart glasses and a ring mouse. (1) While casually walking, AiGet actively analyzes the user's gaze patterns and identifies "interesting" environmental entities. (2) AiGet recognizes the spider lily that the user briefly glanced at as a valuable opportunity for informal learning, considering her interest in plants in the User Profile and lack of knowledge about the flower. (3) Using contextual data, the system generates multiple knowledge candidates and selects the most valuable one about the spider lily for the user. (4) AiGet delivers multimodal feedback, including audio, text keywords with emojis, and an image with a bounding box for the flower to help the user locate the flower and gain information with minimal interference to primary tasks. (5) Surprised by the knowledge delivery about the overlooked flower, the user uses the ring mouse to ask follow-up questions. (6) Later, AiGet continuously helps her uncover design details behind the campus architecture and compare two unfamiliar snacks, aligning with her interest in healthy diets, etc. (7) When the user meets a friend who owns a cat, she shares what she learned from AiGet—that spider lilies are toxic to pets—enriching their social interaction.

such discoveries. Pressed by daily demands, adults focus on primary tasks with higher priority (e.g., rushing for work and studies), bypassing opportunities for curiosity and exploration. We also tend to prioritize immediate rewards, avoiding the perceived effort of learning [\[64\]](#page-20-3). Moreover, the rise of smartphones and social media, which offer effortless access to knowledge, has further diverted our attention from the physical world. As a result, we may overlook rare migratory birds in our backyard while being well-versed in exotic species from online videos; we may enjoy cooking videos about exotic ingredients but remain unaware of unique vegetables like purple broccoli available at the local supermarket.

Emerging technologies like Multimodal Large Language Models (MLLMs) [\[108\]](#page-22-0) and wearable devices, such as smart glasses, offer new possibilities for "having new eyes" to discover hidden knowledge in our environment. By using cameras on smart glasses to sense the surroundings, MLLMs, with their vast knowledge base, can identify relevant information about entities in the environment and proactively deliver it via smart glasses, even when users are engaged in other tasks like commuting. However, this approach faces two challenges: (1) given the multitude of elements that can be explored in our surrounding environment, it remains unclear

which types of knowledge users value most in everyday moments; (2) although proactive knowledge delivery may reduce the effort for manual searches (e.g., using Google or ChatGPT), it could also disrupt primary tasks if not carefully designed. These challenges may hinder the adoption of proactive wearable knowledge discovery assistants. As a result, existing work has largely focused either on user-initiated queries (e.g., users look at a target and ask for information) [\[39,](#page-20-4) [61,](#page-20-5) [100\]](#page-21-1), or limited learning content, such as foreign vocabulary [\[5\]](#page-19-2) or pre-registered material (e.g., AR in museums [\[90\]](#page-21-2)), limiting deeper engagement with the everyday environment.

This leads to our main research question and design goal: How can we utilize wearable intelligent assistants to help users rediscover the hidden yet desired knowledge embedded in the surrounding environment with reduced interference to their primary tasks in everyday settings?

To answer this question, we first conducted a formative study to identify users' learning interests and challenges in everyday contexts. The results revealed four cognitive biases and limitations that hinder users from discovering hidden knowledge: time constraints, inattentional blindness, lack of motivation to explore familiar environments, and a preference for staying within comfort zones

rather than exploring unfamiliar ones [\[11,](#page-19-3) [69,](#page-21-3) [75,](#page-21-4) [84\]](#page-21-5). Despite these challenges, participants consistently expressed enthusiasm for a proactive wearable assistant capable of identifying "unseen" and "unknown" knowledge across different contexts.

Building on these insights, we designed and implemented AiGet, a proactive AI assistant integrated with AR smart glasses, addressing these limitations by seamlessly embedding learning opportunities into daily activities. It analyzes real-time user gaze patterns alongside environmental context and fine-tunes knowledge using personalized user profiles. While existing systems often rely on user-initiated interactions or focus solely on objects in the user's direct line of sight, AiGet extends these capabilities by proactively identifying useful insights about entities in the broader visual field, including areas not under the user's immediate attention. This approach accommodates both "unseen" and "seen but unknown" knowledge, enabling users to discover valuable information they might have otherwise missed.

AiGet's design is carefully tailored for everyday mobile scenarios, ensuring seamless integration alongside primary tasks, with two key aspects: 1) Information Content: To enhance perceived usefulness and reduce annoyance, AiGet prioritizes knowledge that balances novelty, personalization, usefulness, and unexpectedness [\[56,](#page-20-6) [58,](#page-20-7) [88\]](#page-21-6). 2) User Interface Design: To reduce interference and maintain environmental awareness, the system employs multimodal output: keywords and emojis for gist, images with bounding boxes for spatial reference, and audio for details, making knowledge absorption easier. To provide user control while minimizing conflicts during undesired learning moments (e.g., rushing through time-sensitive errands), AiGet uses a ring mouse and subtle gestures for interactions such as turning the system on/off, canceling displays, or initiating follow-up queries. This integrated design naturally embeds learning opportunities into daily activities while remaining unobtrusive and user-friendly.

We conducted in-lab simulations with 12 participants and realworld evaluations involving 18 participants (with 6 for continued usage for up to 7 days) in low-demand daily activities[1,](#page-3-0) such as casual walking, shopping, and museum exploration. AiGet demonstrated effectiveness in uncovering overlooked knowledge in environments, providing information that expands users' knowledge bases while being useful, personalized, and surprising. As a result, it enhanced primary task enjoyment through relevant and timely information delivery, revived users' attention and curiosity about surroundings, deepened their connection with their environment, and demonstrated potential for long-term usability across diverse daily contexts.

Our contributions are threefold: (1) Design insights for analyzing the real-time multimodal environmental and user context to facilitate informal learning in everyday settings, addressing common barriers to spontaneous knowledge acquisition. (2) AiGet, a proofof-concept wearable intelligent assistant carefully designed for mobile everyday usage, transforming daily moments into knowledge discovery opportunities and supporting the acquisition of personalized, context-relevant information. (3) Empirical insights

into how AI-assisted knowledge discovery can enhance engagement with one's environment by identifying "unseen and unknown" knowledge, reviving curiosity, and increasing attention to surroundings, along with design implications for future wearable informal learning assistants.

#### 2 Related Work

This section provides: (1) an overview of learning in daily life and the computing technologies that support it, (2) recent trends in using AI to enhance learning in everyday contexts, and (3) wearable solutions that facilitate learning during daily activities.

### 2.1 Computing Technologies for Learning in Daily Life

Learning occurs not only in formal settings like schools, with instructors and structured curricula, but also informally in daily life, where knowledge and skills are gained through everyday experiences and interactions [\[31,](#page-20-8) [53\]](#page-20-9). Such informal learning can occur either intentionally (a.k.a. self-directed, where learners initiate their learning) or incidentally (a.k.a.serendipitous, where learning occurs unexpectedly or unintentionally, often as a result of chance encounters), at any time and place within daily contexts [\[31\]](#page-20-8). It, thereby, fosters more flexible learning in everyday environments [\[6,](#page-19-0) [72\]](#page-21-0). However, such informal learning poses challenges such as maintaining motivation and engagement [\[91\]](#page-21-7) and balancing learning with the time constraints of daily activities [\[67,](#page-21-8) [72\]](#page-21-0).

Ubiquitous computing devices, including mobile phones andAugmented/Mixed Reality (AR/MR) head-mounted displays (HMDs), have enabled learning at any time and place [\[105\]](#page-21-9), thereby increasing computer-assisted learning opportunities [\[7,](#page-19-4) [62\]](#page-20-10). The sensing capabilities of these devices also allow for contextual awareness, enhancing both intentional and incidental daily learning by aligning content with learners' contexts [\[24,](#page-19-5) [36,](#page-20-11) [43\]](#page-20-12). Supportive systems that leverage context-awareness for daily learning include identifying suitable learning moments [\[24,](#page-19-5) [50,](#page-20-13) [57\]](#page-20-14), delivering context-specific learning content [\[12,](#page-19-6) [35,](#page-20-15) [36,](#page-20-11) [90,](#page-21-2) [98\]](#page-21-10), and providing manageable, situational knowledge [\[62\]](#page-20-10). These approaches have been shown to enhance learners' motivation, engagement, and knowledge retention [\[73,](#page-21-11) [86\]](#page-21-12).

Despite these advances, traditional informal learning systems are often topic- or domain-specific, such as foreign languages [\[43\]](#page-20-12), courses[\[17\]](#page-19-7), or certain daily activitieslike cooking [\[106\]](#page-21-13),restricting their applicability to diverse everyday scenarios. We aim to extend this by utilizing context-aware devices (e.g., AR smart glasses) to support open-world knowledge discovery, addressing constraints such as time and topic limitations through AI-driven technologies.

### 2.2 Leveraging Machine Learning and AI to Enhance Daily Learning

Recent advances in machine learning (ML) and artificial intelligence (AI), particularly in Natural Language Processing and Large Language Models (LLMs, including Multimodal LLMs), have expanded the possibilities for informal learning [\[76\]](#page-21-14). These technologies enable systems to understand natural language queries and generate content on any topic, offering open-world knowledge and adaptability to different learning contexts [\[20,](#page-19-8) [81,](#page-21-15) [107\]](#page-22-1). Examples of such

<span id="page-3-0"></span><sup>1</sup>In our context, we focused on daily tasks with relatively low temporal or cognitive pressure, excluding tasks such as focused work to avoid overwhelming users.

AI-assisted informal learning systems include chatbots and visual question answering (VQA) systems [\[4\]](#page-19-9), which support learning both in digital contexts (e.g., simulated cultural heritage [\[101\]](#page-21-16)) and physical settings (e.g., daily objects [\[33,](#page-20-16) [61,](#page-20-5) [100\]](#page-21-1), everyday activities [\[39\]](#page-20-4), and museums [\[16,](#page-19-10) [80\]](#page-21-17)). These systems are excellent for intentional learning, where users actively seek knowledge, but they often lack support for incidental learning, which we aim to address.

To enhance user experiences, LLM-based systemsincreasingly incorporate personalization through individual user profiles or group personas [\[41,](#page-20-17) [83,](#page-21-18) [92,](#page-21-19) [104\]](#page-21-20). Personalized systems have been shown to increase engagement and motivation for informal learning [\[28,](#page-20-18) [34\]](#page-20-19), yet many LLM/MLLM-based informal learning tools still lack personalization capabilities (with notable exceptions like the proposed G-VOILA framework [\[100\]](#page-21-1), yet without implementing this feature for real-world evaluation). While we envision integrating personalization with LLM-based systems could enhance informal knowledge generation, how to accommodate both long-term preferences and real-time context remains underexplored, especially for incidental learning scenarios where users lack explicit learning intentions. In this work, we investigate how personalized MLLM-based systems can leverage context awareness to enhance the informal learning experience during daily activities.

### 2.3 Wearable AI Assistants for Everyday Learning

Wearable devices offer greater convenience and continuous use compared to traditional computing devices across various domains [\[54\]](#page-20-20). They enable new paradigms, such as wearable computing [\[30\]](#page-20-21) and heads-up computing [\[111\]](#page-22-2), which provide real-time assistance during daily activities, including learning [\[2,](#page-19-11) [29,](#page-20-22) [51\]](#page-20-23). In particular, AR/MR HMDs (a.k.a. AR smart glasses) display digital information overlaid on the physical world [\[8,](#page-19-12) [14\]](#page-19-13), enhancing context-aware learning experiences [\[23,](#page-19-14) [60,](#page-20-24) [68,](#page-21-21) [98,](#page-21-10) [102\]](#page-21-22).

The integration of LLM/MLLM-based AI into wearable devices, coupled with in-situ context sensing [\[37,](#page-20-25) [40\]](#page-20-26), unlocks new opportunities for informal learning [\[39,](#page-20-4) [51,](#page-20-23) [61,](#page-20-5) [100\]](#page-21-1). For example, Google Project Astra [\[39\]](#page-20-4) allows users to learn about physical objects and activitiesthrough natural interactions(e.g., voice or gesture queries) using AR HMDs. Similarly, GazePointAR [\[61\]](#page-20-5) and G-VOILA [\[100\]](#page-21-1) use gaze-assisted voice queries to facilitate knowledge acquisition in situ. While these systems facilitate intentional learning on any topic, they do not address incidental learning. For instance, Gaze-PointAR [\[61\]](#page-20-5) and G-VOILA [\[100\]](#page-21-1) utilize gaze to identify "areas of interest/referential pointing" but rely on explicit verbal queries to retrieve desired information. In contrast, adopting a "desire prediction" approach [\[25,](#page-19-15) [27,](#page-19-16) [63\]](#page-20-27), we aim to leverage gaze data not only to detect focus levels on environmental entities but also to infer user mental states and implicit learning desires through user profile analysis. Additionally, we strive to extend beyond objects directly gazed at, delivering valuable knowledge about peripheral or overlooked entities at opportune moments. This query-free, contextaware approach facilitates seamless incidental learning.

In this work, we introduce AiGet, a proactive wearable AI assistant that supports both AI-initiated and user-initiated multimodal interactions for informal knowledge acquisition in daily life. Additionally, we explore how the continued use of AI-assisted informal

learning, specifically for incidental learning, impacts learning experiences across diverse scenarios.

#### 3 Study Overview

Our research began with a formative study to understand users' knowledge acquisition needs in daily life. Subsequently, we developed the proof-of-concept system, AiGet. We then conducted an in-lab evaluation to assess AiGet's ability to generate desirable knowledge and a real-world study to evaluate its feasibility and collect additional design insights. All studies were approved by our university's institutional review board (IRB), and participants were compensated ≈ 7.5 USD per hour, a standard rate for user studies in the local context.

### 4 Formative Study: Understanding Informal Learning Desires during Daily Activities

We envision a wearable AI assistant that proactively provides relevant, in-situ knowledge based on the user's environment. However, a key question remains: RQ: What features do users expect from a wearable AI assistant to support their knowledge acquisition during daily activities?

To address this question in this formative study, we explore the main barriers that prevent users from discovering informal knowledge in their everyday environments, identify the types of knowledge users desire, and conclude with four design goals to satisfy users' expectations for informal learning with wearable AI assistants.

#### 4.1 Participants

We recruited twelve volunteers (P1-P12, 7 females, 5 males, Age: = 24.9, = 3.8 years) from the university community, all self-reporting professional working fluency in English. To ensure the accuracy of our eye-tracking equipment, we selected participants with normal or corrected vision, excluding those wearing spectacles. Among the twelve participants, six reported using VQA apps or search engines (e.g., Google Lens) to query objects/entities in daily life at least twice per day, five used such tools with medium frequency (3-7 times per week), and one participant rarely used these technologies.

#### <span id="page-4-1"></span>4.2 Apparatus

Following prior research [\[15,](#page-19-17) [22,](#page-19-18) [25\]](#page-19-15), to capture users' natural behavior during their daily routines, we utilized portable devices to record users' visual experiences and behaviors. Participants wore a backpack containing a laptop (MacBook Pro 14-inch with M2 Pro chip), which collected all the recordings. Their visual experiences, including gaze patterns from a first-person view (FPV), were recorded using a Pupil Core[2](#page-4-0) eye tracker (World Camera: 30Hz, 1080p, FoV: 139°×83°; Eye Cameras: 120Hz) connected to the laptop. An accompanying experimenter, maintaining a distance to avoid interference while ensuring participants' safety, recorded participants' actions using a mobile phone (iPhone 12) from a third-person

<span id="page-4-0"></span>[<sup>2</sup>https://pupil-labs.com/products/core/](https://pupil-labs.com/products/core/)

view (TPV). This TPV feed was streamed via Zoom. Zoom synchronized and recorded the FPV, gaze, and TPV, enabling playback for reviewing recorded experiences later.

#### 4.3 Study Design and Procedure

To explore user behavior and learning desires in daily contexts, we designed a study that recorded participants' routines across three settings: casual walking in indoor campus areas, outdoor campus areas, and shopping. These settings allowed us to capture informal learning opportunities in environments with both familiar (e.g., buildings on campus) and unfamiliar entities (e.g., stores with imported products). They also represented a range of information density: low (e.g., outdoor campus), medium (e.g., indoor campus with posters and exhibitions), and high (e.g., stores with a wide variety of products).

The study was conducted in three phases: 1) Pre-Study Interview: Participants were interviewed to understand their daily routines, informal learning habits, challenges, and desired learning goals. 2) Natural Behavior Recording: Participants followed a designated path (indoor to outdoor campus to a market with unfamiliar imported products) while their behaviors were recorded using the apparatus (Sec [4.2\)](#page-4-1), without experimenter intervention. 3) Think-Aloud and Reflection: Participants retraced the path, thinking aloud about the knowledge they wanted and why. After completing the path, they reviewed their FPV and gaze recordings at 2x speed, annotating desired knowledge points [\[47\]](#page-20-28). To understand if any knowledge was ignored yet desirable, a "Wizard of Oz" simulation using GPT-4V provided overlooked knowledge for familiar and unfamiliar entities (e.g., animals, plants, products) and both attended and ignored items. Participants' expectations for wearable AI interactions were also documented.

#### <span id="page-5-5"></span>4.4 Data Analysis

Upon consolidating 12 transcribed interview notes with observational notes detailing user behaviors and environments (i.e., desireto-know moments with remarks, including what and why to learn, challenges with prior learning, and envisioned AI assistance), we employed a thematic analysis as outlined by Braun and Clarke [\[18\]](#page-19-19) (detailed in Appendix [A.1\)](#page-22-3).

#### 4.5 Findings

<span id="page-5-3"></span>4.5.1 Four Common Barriers Limiting Users from Discovering Informal Knowledge in Daily Activities. Participants, primarily university students and staff, described weekday routines centered on commuting, work, and occasional sports, with weekends involving more personal activities like shopping and traveling. Within these routines, participants identified four main barriers to learning in their daily environments. (1) Time and Attention Limitation [\[11,](#page-19-3) [21\]](#page-19-20): Aligning with informal learning literature [\[67,](#page-21-8) [72\]](#page-21-0), all participants mentioned this as the most significant barrier. They are aware of missing out on new information because they focus on existing tasks and feel that the effort required to seek out new information outweighs the potential benefits, leading to "Known Unknowns"[3.](#page-5-0) (2) Inattentional Blindness [\[69\]](#page-21-3): Participants often failed to notice

interesting elements in their environment due to cognitive constraints, such as being preoccupied with other tasks or work-related thoughts, leading to "Unseen" knowledge. (3) Illusion of Explanatory Depth [\[84\]](#page-21-5): Participants overestimate their understanding or familiarity with surrounding entities, leading to "Unknown Unknowns"[4](#page-5-1) unless pointed out by others (e.g., "I never knew the commonly seen tree is the national tree here and people used it for cutting boards. (P2)" (4) Uncertainty Avoidance [\[75\]](#page-21-4): Barriers like language gaps lead users to avoid exploring unfamiliar environments or subjects (e.g., "unfamiliar imported food in stores"), also resulting in "Known Unknowns"[5.](#page-5-2) Although these barriers hinder knowledge discovery in daily life, P6 noted that they do not signify a loss of curiosity. Instead, they tend to make quick assumptions about unfamiliar things (e.g., unfamiliar fruit taste) to save time, even if those assumptions are sometimes incorrect.

Meanwhile, participants have shifted their primary informal knowledge acquisition to passive consumption through social media platforms like Twitter and YouTube, aligning with prior research [\[44,](#page-20-29) [78\]](#page-21-24). This behavior contrasts with their past, particularly in childhood, when they actively questioned their surroundings. Now, they only seek in-situ knowledge under specific conditions: when highly interested and with ample free time (e.g., "I used plant recognition apps to search for very unique flowers I encountered (P9)"), or when the information has high practical value (e.g., "I will search for tips before traveling or going to exhibitions (P2)").

<span id="page-5-4"></span>4.5.2 Desired Types of Knowledge for Addressing Informal Learning Barriers. To overcome Time and Attention Limitation, participants expressed a desire for proactive AI to provide knowledge with little effort during daily moments. They emphasized that such a system should offer contextually relevant knowledge. Revised Bloom's Taxonomy [\[59\]](#page-20-30) categorizes four types of knowledge: Factual, Conceptual, Procedural, and Metacognitive (see details in Appendix [A.2\)](#page-22-4). During daily activities, Metacognitive knowledge is less desirable, as it demands complex thinking and could exacerbate Time and Attention Constraints. As P8 noted, "If it prompts me to think about milk safety in different containers while shopping, it's too much. I'd rather review that information later."

For the remaining three categories, we found that their preferences are linked to the entity's familiarity level, echoing the need to address Illusion of Explanatory Depth and Uncertainty Avoidance (Sec [4.5.1\)](#page-5-3).

For Commonly Seen or Familiar entities that lead to Illusion of Explanatory Depth, participants preferred interesting factual knowledge offering surprises (e.g., "shocking facts or statistics about environmental issues associated with plastic Coca-Cola bottles (P10)") or useful procedural tips relevant to ongoing or future tasks (e.g., "how to mix common beverages for unique flavors (P7)"). For Unfamiliar entities, which lead to Uncertainty Avoidance, participants found basic introductions (factual knowledge) in an easy-to-understand format most helpful. Additionally, P3 and P4 appreciated when unfamiliar entities were linked to familiar concepts, making new information more relatable and easier to grasp.

<span id="page-5-0"></span><sup>3</sup>Things we are aware of but don't understand, from Rumsfeld Matrix [\[85\]](#page-21-23)

<span id="page-5-1"></span><sup>4</sup>Things we are neither aware of nor understand 5Things we are aware of but don't understand

<span id="page-5-2"></span>

We also found that knowledge preference regarding the Focus Level of the Entity, i.e., whether the provided knowledge relates to what users are actively observing, can partially mitigate the effects of Inattentional Blindness. These preferences were related to observed three gaze and motion patterns, similar to prior literature [\[50\]](#page-20-13),revealing users' varying interests based on their environmental engagement and focus:

(1) When users show a Saccade gaze pattern (i.e., random gaze movements across different entities in FPV) during casual walking, their mentalstate often suggeststhat they are looking forsomething interesting, relaxing their mind, or thinking about unrelated tasks. In these cases, participants appreciated fun, factual knowledge about their surroundings, regardless of whether the associated entity is scanned (i.e., "unnoticed interest is also welcomed (P4)"). However, four participants emphasized that if they were deep in thought, which is not reflected in gaze patterns alone, they would likely turn off the assistant to avoid interruptions.

(2) During Quick Browse (i.e., rapid gaze scanning of related entities in FPV), where users scan familiar entities (e.g., store shelves), they have a medium interest level in the scanned target and typically are seeking something more engaging. In this context, providing interesting knowledge related to the scanned entities can enhance users' interest in familiar items.

(3) When users are fully Focused (i.e., gaze fixations on specific entities in FPV), it indicates high interest, often because the entity is unfamiliar, they want to learn more about it, or they are making a decision. In such moments, participants found it helpful to receive factual and conceptual knowledge that builds connections or comparisons between the primary focused entity and similar nearby entities. They also appreciated comparisons between the entity and something familiar, even if not physically present. For example, when P3 found pre-made breakfast in the store, they wanted to know what made these options unique compared to similar ones in their hometown. Such comparisons provide a familiar context, making it easier for users to understand and engage with new information.

<span id="page-6-0"></span>4.5.3 Summarized Design Requirements for Wearable AI Assistant to Help Users Learn from Environment. Given the identified barriers, user preferences for knowledge acquisition, and considering the unique challenges of information intake in mobile settings, we synthesized four design requirements for a wearable AI assistant based on participant feedback.

[D1](#page-6-0): Analyze Context and Provide "Unseen" and "Unknowns" in Real Time. Building on the insights for varying knowledge desires under different contexts, participants envision the AI assistant analyzing their environment and gaze behavior to predict their primary activities and potential learning desires, subsequently providing knowledge support about their "unnoticed" or "unknown interesting knowledge" about the in-situ environment. In addition to physical contexts, the assistant could check digital contexts, like the "agenda", to predict support for follow-up activities, such as offering guidance on suitable food and drinks before exercising when shopping. In addition, the participants mentioned the need to support human initiative queries in case "the AI doesn't know what exactly I want to know. (P1)"

[D2](#page-6-0): Personalization is Needed to Fine-Tune Knowledge Content. Participants emphasized the need for personalization, noting that it enables the AI assistant to understand their long-term interests and familiarity level with various entities in their environment. By assessing familiarity, the assistant can tailor knowledge types and expertise levels (Sec [4.5.2\)](#page-5-4). To achieve this, they suggested collecting a "basic user profile like social media platforms did when registering (P3)". While personalization is valued, participants also expressed concerns about "the system constantly suggesting information based on past interests, similar to social media algorithms (P2)", which could become annoying. Instead, they hope the assistant utilizes this familiarity to introduce new perspectives and noteworthy entities in daily activities that might otherwise be overlooked.

<span id="page-6-1"></span>[D3](#page-6-0): Knowledge Prioritization to Provide the Most Valuable Information. Participants appreciated discovering information from their environment but emphasized the need to prioritize the most valuable content, avoiding information that is trivial. To address this, we propose a multi-factor weighting system to prioritize knowledge selection. First, Novelty was considered crucial, with most participants (10 out of 12) agreeing that the knowledge must be new to them, either by introducing entirely new perspectives or by adding unknown details to common knowledge. For example, instead of simply stating that "palm trees provide habitats for various species," the assistant could highlight that "palm trees provide habitats for species like the Palm Weevil, which lays its eggs in the trunk." Additionally, participants identified three factors as beneficial, though not always necessary: (1) Alignment with Personal Interests and Values, such as providing bargain-hunting tips for participants who value "saving"; (2) Usefulness for ongoing or future tasks, like helping health-oriented users compare sugar levels when buying drinks; and (3) Unexpected Perspectives that offer surprising insights or correct misconceptions, such as "zero-sugar drinks do not mean they contain zero calories."

These priorities align with existing literature on user information-seeking behavior, which emphasizes information utility [\[56,](#page-20-6) [88\]](#page-21-6), and are consistent with measures of serendipity in recommender systems [\[58\]](#page-20-7), focusing on novelty, unexpectedness, and relevance. By integrating these elementsinto the weighting formula: Novelty × (AlignUserPreference + Utility + Unexpectedness), the assistant can prioritize the most valuable knowledge for users.

[D4](#page-6-0): Minimize Distraction to Primary Tasks in Mobile Settings. Given the use of the wearable assistant in scenarios where users have other primary tasks and consume information on the move, participants highlighted two main points to minimize distraction. First, regarding knowledge delivery, participants emphasized the need to minimize the cognitive load during receptive moments for learning. As P7 noted, "the content should be concise and easy to digest, like a podcast." They also emphasized that the assistant should provide only 1-2 pieces of information at a time to prevent overwhelming users, especially during tasks requiring focus, unless they explicitly requested more. Second, concerning system control, participants emphasized the importance of accommodating moments when users are unwilling to receive knowledge. This includes allowing users to turn the assistant off as needed for extended periods (e.g., while rushing to a destination or preoccupied

with work or study) or easily interrupt single information delivery if the content is unsatisfactory. These considerations aim to ensure that the assistant enhances the user experience rather than hinders the user's primary activities in mobile settings.

Based on these design requirements, we propose a framework for desired knowledge generation, as illustrated in Figure [2.](#page-8-0) This framework outlines the process of transforming daily moments into valuable learning opportunities while minimizing distractions and incorporating user feedback.

#### <span id="page-7-1"></span>5 AiGet System

We introduce AiGet, a proof-of-concept system that aligns with the aforementioned design goals (Sec [4.5.3\)](#page-6-0). In this section, we detail the primary features of the AiGet system, its implementation, and an in-lab simulated ablation study conducted to evaluate its knowledge generation capability.

#### 5.1 Key Features of AiGet System

The AiGet system pipeline, as shown in Figure [3](#page-8-1), consists of five key stages: (1) LLM Request Trigger, (2) Context Analysis, (3) Knowledge Generation and Prioritization, (4) Output Transformation, and (5) Follow-up User Actions. It was developed following the design goals (Sec [4.5.3\)](#page-6-0) and (iterative) pilot testing with eight users from the university, all with backgrounds in UI/UX design or HCI.

<span id="page-7-3"></span>5.1.1 LLM Request Trigger ([D1](#page-6-0)). Aligning with [D1](#page-6-0), to support both proactive AI prompting and user-initiated queries to discover "unseen" and "unknowns", AiGet adopts a mixed-initiative approach [\[3,](#page-19-21) [45\]](#page-20-31) to trigger LLM requests (Figure [3](#page-8-1) (1)).

AI-Initiative: Constant Sensing. Since users may not always notice all the interesting entities in their environment and be aware of the "unknown knowledge" associated with them, AiGet continuously monitors the environment (via FPV camera) to identify hidden yet desired knowledge from surroundings. To prevent information overload and repetition, LLM requests are triggered only when both a minimum time interval (12 seconds) has passed and a significant FPV difference threshold is met (see Appendix [B.1](#page-22-5) for technical details). This ensures users have enough time to process each piece of generated knowledge on mobile situations.

AI-Initiative: Implicit Gaze Fixation. In addition to constant sensing, AiGet monitors the user's gaze pattern to trigger LLM requests. When AiGet detects user fixation patterns (eyes focused on a small area, deviating no more than 4.91 degrees for at least 1 second, as suggested by prior research [\[25\]](#page-19-15)), it indicates potential interest in the target object and triggers an LLM request.

User-Initiative: Explicit Verbal Query. Recognizing that users may have specific questions during daily activities or may want to ask follow-up questions after being inspired by AI's proactive suggestions (i.e., intentional learning), AiGet supports explicit verbal queries from users, similar to the prior visual Q&A system [\[61,](#page-20-5) [100\]](#page-21-1). To prevent false triggers in noisy environments, users can press the right button on the handheld ring mouse to control their query [\[25\]](#page-19-15). The system then incorporates the user's verbal comments or questions into the LLM request.

<span id="page-7-2"></span>5.1.2 Context Analysis ([D1](#page-6-0), [D2](#page-6-0)). When an LLM request is triggered, AiGet compiles multimodal contextual data, including the user's FPV image frames with gaze data, time, location, and any user query transcriptions for analysis.

To support [D1](#page-6-0), AiGet employs a Context Analysis Agent (Figure [3](#page-8-1) (2)) to extract meaningful contexts by analyzing short-term attention and long-term interests. This enables the system to predict learning desires and minimize the generation of irrelevant or low-quality knowledge from extensive raw multimodal data, particularly when explicit user learning desires are not expressed. Specifically, the agent recognizes user activity, labels entities, predicts user intentions, and identifies potential unseen or unknown entities and knowledge topics aligned with the user's interests.

To help the LLM understand the user's primary activities and gaze patterns, AiGet overlays gaze positions on each FPV frame with red circles, enabling LLM to discern focused regions while maintaining global context [\[89,](#page-21-25) [107\]](#page-22-1). The LLM then analyzes the user's activities, identifies objects in both primary and peripheral visual fields, and interprets gaze patterns (Sec [4.5.2\)](#page-5-4) using predefined rules: Saccade (random gaze movements across different entities in different frames), Quick Browse (rapid scanning of related objects in different frames), and Focused (sustained attention on specific objects in most frames). The LLM also predicts potential learning desires based on these patterns and contextual information, such as "looking for a specific keyboard model or comparing different keyboard options" when users focus on various keyboards in stores.

To enhance analysis and support [D2](#page-6-0), the system provides the LLM with a user profile (Figure [3](#page-8-1) (User Profile)) [\[100,](#page-21-1) [104\]](#page-21-20) containing values, interest[s6,](#page-7-0) and basic demographic information (i.e., age, gender, nationality, residence, and academic/professional background; see Appendix [B.2](#page-22-6) for a sample profile). Using this profile, the LLM assesses the user's familiarity with the current location and surrounding entities (e.g., "a senior student is familiar with the campus," or "coffee lovers know the basics of different types of coffee"). It also refines learning desires and identifies potential connections between the environment and the user's interests (e.g., linking the topic of "fruit nutrition" to a user's interest in a "healthy lifestyle" when passing the supermarket fruit aisle).

Prompt Improvement Through Iterative Design. We identified two key challenges in accurately identifying entities from long and raw multimodal inputs in real-world scenarios: ambiguity when multiple objects overlap in gaze direction and visual recognition errors for underrepresented entities in image training datasets. To address these issues, we employed a multimodal chain-of-thought reasoning approach [\[110\]](#page-22-7), as detailed in Table [1,](#page-9-0) which significantly improved contextual accuracy in incidental learning during our pilot tests.

<span id="page-7-4"></span>5.1.3 Knowledge Generation and Prioritization ([D1](#page-6-0), [D2](#page-6-0), [D3](#page-6-0)). After context analysis, the Knowledge Generation Agent (Figure [3](#page-8-1) (3)) generates and selects the most "desired" knowledge candidates

<span id="page-7-0"></span><sup>6</sup>To ensure broad coverage of user interests and values, we followed a two-step process: (1) Curating 14 informal knowledge topics (e.g., Animals & Plants) and 58 value categories (e.g., Physical Value-Beauty) from taxonomies in prior literature [\[46,](#page-20-32) [66,](#page-21-26) [71,](#page-21-27) [87\]](#page-21-28) to guide participant selection; (2) Encouraging participants to expand the list with detailed topics (e.g., "Plant Care Tips") that aligned with their experiences and learning desires.

#### CHI '25, April 26–May 01, 2025, Yokohama, Japan Cai et al.

<span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)

Figure 2: Proposed Framework for Generating Desired Knowledge from Daily Moments. Note: Gray dashed boxes and lines represent user suggestions for future system improvements, collected from the final real-world study (Sec [7.2\)](#page-17-0) and not implemented or evaluated in the current paper. Our implementation, AiGet, is detailed in system design (Sec [5\)](#page-7-1).

<span id="page-8-1"></span>![](_page_8_Figure_3.jpeg)

Figure 3: AiGet's System Processing Pipeline. (1) LLM Request Trigger: Supports mixed-initiative knowledge queries. (2) Context Analysis: Analyzes real-time user attention and long-term interests and infers learning desires. (3) Knowledge Generation & Prioritization: Filters redundant content, prioritizing the most valuable knowledge to avoid overload. (4) Output Transformation: Presents knowledge in a multimodal format to balance engagement and cognitive load. (5) Follow-Up User Actions: Enables user control over interaction. Some details are omitted in the figure. Full details and agent prompts are in Appendix [C.](#page-22-8)

based on the contextual description[7,](#page-8-2) interaction history, and knowledge value.

Aligning with [D1](#page-6-0) and [D2](#page-6-0), the agent produces factual, conceptual, and procedural knowledge on both primary and peripheral entities. The knowledge content is required to be novel, useful, and surprising, avoiding repetitive topics by checking history. The agent

<span id="page-8-2"></span><sup>7</sup>It combines an FPV image with textual data from the Context Analysis Agent to balance visual detail retention and input token efficiency.

also fine-tunes content based on user familiarity and interest levels, e.g., offering advanced insights on familiar topics and linking new information to known concepts for unfamiliar ones.

To reduce trivial knowledge provision ([D3](#page-6-0)), we use Chain of Thought methods [\[103\]](#page-21-29), instructing the LLM to score each knowledge item using the prioritization formula (sec [4.5.3\)](#page-6-1): Novelty × (AlignUserPreference + Utility + Unexpectedness), with each factor marked as 0 or 1 by the agent with reasoning. Only knowledge content scoring >= 2 is retained, based on pilot results. The LLM then selects the suitable knowledge based on the user's gaze pattern mode (Sec [4.5.2\)](#page-5-4), following rules from the formative study (e.g., for Focused mode, providing comparative knowledge about primary and related peripheral entities). No more than two concise knowledge items are presented at a time to minimize information overload.

Prompt Improvement Through Iterative Design. Designing incidental learning for mobile settings posed unique challenges in generating novel and unexpected content due to ambiguous instructions and LLM limitations in processing long contexts [\[70\]](#page-21-30). To overcome these challenges, we refined multiple prompt strategies through iterative pilot testing, as summarized in Table [2.](#page-10-0)

5.1.4 Output Transformation ([D4](#page-6-0)). To align with [D4](#page-6-0) and pilot testing results, AiGet delivers output in a multimodal format (e.g., Figure [3](#page-8-1) (4), Figure [8\)](#page-14-0), balancing minimal distraction with increased engagement by reducing the cognitive load associated with information intake. The system integrates three key features: 1) Audio modality to provide full knowledge content, enhancing engagement while minimizing text reading efforts on the move [\[95,](#page-21-31) [96\]](#page-21-32); 2) Emojis/pictograms and text keywords to succinctly convey the content gist and enhance comprehension [\[52\]](#page-20-33); and 3) Images with highlighted bounding boxes around mentioned entities to provide visual references, particularly for overlooked or unfamiliar entities. All visual information is presented in the user's peripheral vision areas, following attention-maintaining interface guidelines [\[26,](#page-19-22) [49\]](#page-20-34).

To optimize processing time, two agents work in parallel during output transformation, as detailed in Appendix [B.3.](#page-22-9)

5.1.5 Follow-Up User Actions ([D4](#page-6-0)). After receiving system output, users can perform follow-up actions using the ring mouse (Figure [3](#page-8-1) (5)). To align with [D4](#page-6-0) and accommodate moments when users prefer not to engage, pressing the up button cancelsthe latest knowledge display, stopping audio and hiding visual information (Emojis and Text and Image Reference) when encountering unwanted information. The left button mutes or unmutes Audio feedback, while the bottom button toggles the system on or off for extended periods, controlling proactive AI suggestions. As mentioned earlier, the right button enables users - to initiate follow-up queries. The buttons and UI elements are spatially - mapped for easy interaction [\[26,](#page-19-22) [77\]](#page-21-33).

#### <span id="page-9-3"></span>5.2 Apparatus and Implementations

An XReal Air[8](#page-9-1) was used as the near-eye AR display, with the Pupil Core add-on for gaze detection and FPV streaming [\[25\]](#page-19-15). The smart glasses were connected to a laptop (MacBook Pro 14-inch, M2 Pro chip), which served as the computing unit to display the AiGet UI. A Sanwa Supply Bluetooth Ring Mouse (MA-BTRING3B[K9\)](#page-9-2) was used to control the UI.

Python was used for both the frontend and backend, utilizing a TKinter-based interface to seamlessly handle real-time capture and concurrent processing of various context data and user interactions. For the LLM pipeline, we employed the Gemini-series models (i.e., Gemini-1.5-Flash for contextual analysis and output transformation, and Gemini-1.5-Pro for knowledge generation). These models were selected due to the support of 1) large context windows, enabling the system to process multiple contextual inputs and maintain a long knowledge history, and 2) safety settings to avoid potential ethics issues with generated content. Detailed system implementation and prompt information can be found in Appendix [B.4](#page-22-10) and Appendix [C.](#page-22-8)

### 5.3 In-Lab Evaluation of AiGet's Knowledge Generation Pipeline

To verify whether the AiGet pipeline can generate desired knowledge using multimodal contextual data and to determine which aspects of our design goals (Sec [4.5.3\)](#page-6-0) contribute to such knowledge generation, we conducted an in-lab simulated evaluation.

Specifically, we conducted an ablation study by removing two key components of AiGet pipeline—knowledge generation rules ([D3](#page-6-0)) and personalization ([D2](#page-6-0))—from the pipeline, following the method used in prior research [\[100,](#page-21-1) [104\]](#page-21-20). The pipeline originally consisted of a Multimodal LLM, User Profile, and Rules (Prioritization with Focus/Familiarity Analysis). We kept the Multimodal LLM ([D1](#page-6-0)) as it is compulsory for context-relevant knowledge generation. Then, we compared the knowledge generated by the three resulting pipelines to understand what components and associated guidelines affect the desirability of knowledge. Note: [D4](#page-6-0) is not considered here, as it focuses on interaction rather than knowledge generation.

5.3.1 Two Baselines.

<span id="page-9-0"></span>

| Challenges                 | Solutions & Steps                         |
|----------------------------|-------------------------------------------|
| Ambiguity in Gaze          | Context-Guided<br>Selections:<br>The      |
| Interpretation: When       | MLLM was instructed to describe user      |
| gaze direction overlaps    | activity first and prioritize elements    |
| multiple entities.         | based on rarity and uniqueness to the     |
| Example:<br>Distinguish    | scene and context.                        |
| ing between a glass jar    | Example: As emphasized by P1: "I can      |
| and the specimen inside.   | learn about tempered glass elsewhere, but |
|                            | the specimen is unique to the museum."    |
| Visual<br>Recognition      | Multimodal-Facilitated Visual Pro         |
| Errors: For underrep       | cessing: When images alone were in        |
| resented<br>entities<br>in | sufficient, the MLLM performed OCR to     |
| image training dataset.    | extract textual cues in the scene, enhanc |
| Example:<br>A<br>newly     | ing entity recognition via text labels.   |
| released Asian drink.      | Example: Extracting labels on bottles and |
|                            | shelves helped the MLLM identify the      |
|                            | drink accurately.                         |

<span id="page-9-1"></span>[<sup>8</sup>https://www.xreal.com/air](https://www.xreal.com/air)

<span id="page-9-2"></span>[<sup>9</sup>https://www.sanwa.co.jp/product/syohin?code=MA-BTRING3BK](https://www.sanwa.co.jp/product/syohin?code=MA-BTRING3BK)

<span id="page-10-0"></span>

| Aspect         | Challenges                                                                                                                                                                                                                                                                                                  | Solutions & Steps                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Novelty        | Balancing Detail with Conciseness:<br>• Requirement for Concisenessled to generic out<br>puts from naive LLMs, lacking depth for edu<br>cated users.<br>• Example: "Palm trees play an important role<br>in the ecosystem by providing food for various<br>species."                                        | Few Shot Prompting:<br>• Employed few-shot prompting [82] with concrete examples to guide LLMs<br>in tailoring details to users' education levels.<br>• Example: "Palm trees emit volatile organic compounds attracting beneficial<br>insects, reducing pesticide use." balances conciseness with novel details, and<br>domain experts could receive more technical precision.                                                                                                                                                                                                                                                              |
|                | Reducing Repetition when Revisiting Enti<br>ties:<br>• Content repetition undermined novelty, espe<br>cially when revisiting recurring entities like<br>trees along a route.<br>• Simply feeding full histories and requiring new<br>content failed due to LLMs' limitations in long<br>context processing. | Filtering Mechanism with Chain of Thoughts:<br>• Inspired by retrieval-augmented generation (RAG) [109], only fed the 10<br>most relevant historical items based on semantic similarity* to contextual<br>description.<br>• Applied Chain-of-Thought method [103]to break complex tasksinto smaller<br>steps and maximize the likelihood of generating novel content:<br>– Identify entities/topics already mentioned in concise history.<br>– Determine unexplored gaps.<br>– Generate content focusing on new perspectives or entities.<br>• Post-generation, filter content highly similar to history (≥ 0.75)*<br>to ensure<br>novelty. |
| Unexpectedness | Lack of Surprising Perspectives:<br>• Hard to generate unexpected content without<br>explicit guidance.                                                                                                                                                                                                     | Specifying Potential Unexpectedness Category:<br>• Guided by "surprise taxonomy" [74], prompts suggested:<br>– Unseen but interesting entities (Observation-Mismatch Surprise [74]).<br>– Lesser-Known Fun facts (e.g., trivia) or misconceptions (Belief-Mismatch<br>Surprise [74]).                                                                                                                                                                                                                                                                                                                                                       |

#### Table 2: LLM Prompt Refinements to Address Novelty and Unexpectedness in Content Generation

\* The similarity is calculated using the all-MiniLM-L6-v2 model, <https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2>

Baseline w/o R (i.e., Multimodal LLM + Profile). This baseline omits the Rules that associate knowledge categories with gaze pattern and familiarity analysis while retaining the user profile in the prompt. The LLM is instructed to analyze multimodal input data (FPV + gaze overlay, location, and time) and generate knowledge that 'enhances interest, expands knowledge, and includes serendipitous information' following our original objectives (Sec [5\)](#page-7-1), tailored to the user's profile when possible. It's directed to produce no more than two knowledge items, avoiding basic information that neither broadens the user's perspective nor provides utility (but is not required to calculate specific scores as AiGet did). To assess whether basic user profiles could lead to more personalized and valuable knowledge, both AiGet and Baseline w/o R use identical user profiles, as mentioned in Sec [5.1.2.](#page-7-2)

Baseline w/o RP (i.e., Multimodal LLM only). This version removes both the Rules and the Personalized user profile. Similar to Baseline w/o R, but without the benefit of a user profile, this pipeline generates knowledge based solely on multimodal input data. It follows the same guidelines for selecting relevant knowledge and avoiding basic information.

To ensure a fair comparison, all baselines maintain a similar prompt structure, prompt engineering methods, and LLM models for knowledge generation as AiGet, except for the aforementioned differences. Note that we didn't implement a version that "only removes personalization while keeping the rules", as the rules depend on user profiles. Thus, both components are omitted in Baseline

w/o RP, aligning with the prior research's method [\[104\]](#page-21-20). Detailed prompts for both baselines are in Appendix [C.4](#page-22-12) and [C.5.](#page-22-13)

5.3.2 Participants. As the perception of generated knowledge is subjective and context-dependent, to have a realistic comparison between the AiGet pipeline and baseline pipelines, participants rated the knowledge generated by three pipelines using their own recorded daily activities, where they had first-hand experience.

Thus, we invited 12 participants from the formative study to evaluate knowledge generated from their previous daily activity recordings. This approach enabled personalized evaluation based on participants' actual experiences, allowed them to judge knowledge accurately, and improved resource efficiency by eliminating the need for new recordings.

5.3.3 Study Design and Data Preparation. A within-subject design was used to compare the knowledge generation quality across different pipelines. Each participant rated knowledge generated from 9 moments, with 3 knowledge items per moment (one from each pipeline), totaling 27 knowledge items per participant.

Moments Selection. For each scenario from the formative study (i.e., indoor and outdoor casual walking on campus and shopping), three moments with different scenes (selected based on FPV similarity, detailed in Sec [5.1.1](#page-7-3) and Appendix [B.1\)](#page-22-5) were randomly chosen to balance variety and avoid overwhelming participants when evaluating multiple aspects of each pipeline. Although not all possible generated knowledge items were assessed, the selected samples realistically estimated each pipeline's performance. In total, 108

data samples were selected for evaluation (3 moments per scenario × 3 scenarios per participant × 12 participants).

Knowledge Generation. For each moment, knowledge was generated using all three pipelines and presented in randomized order with anonymous ID (i.e., Knowledge 1-3) to ensure unbiased evaluations. See Figure [4](#page-12-0) for an example.

<span id="page-11-0"></span>5.3.4 Measures. We assessed the desirability of the generated knowledge across several key aspects using subjective measures, following prior research [\[56,](#page-20-6) [58,](#page-20-7) [88,](#page-21-6) [90\]](#page-21-2), including Novelty, Personalization, serendipity (encompassing Usefulness, Unexpectedness, and Relevance), Interesting, and Deepen Topic Understanding—each measured using a 7-point Likert scale (1 for "Strongly Disagree" and 7 for "Strongly Agree"). For details, see Appendix [D.1.](#page-23-0)

Additionally, to evaluate whether the information was appropriate for presentation during primary tasks [\[25,](#page-19-15) [26,](#page-19-22) [48\]](#page-20-35), we collected Overall Perceived Scores for receiving such knowledge during primary tasks, Not Annoying, and Not Overload, also using 7-point Likert scales.

Moreover, we also recorded instances where the MLLM pipeline did not generate any knowledge because it determined nothing valuable to provide (Note: for these moments, participants only rated three measures: Overall Perceived Scores, Not Annoying, and Not Overload).

Analysis. A Wilcoxon signed-rank test and descriptive statistics were used to analyze the data.

5.3.5 Results. As shown in Figure [5,](#page-12-1) AiGet pipeline achieved significantly higher scores ( < 0.05) across all measures compared to the two baselines, with all scores above 5 out of 7. This demonstrates its ability to generate novel, personalized knowledge with serendipity while keeping the information non-annoying and nonoverloading. There was no significant difference between the two baselines except in Personalization ( < 0.05).

Impact of User Profile Alone: Comparing Baseline w/o R vs. Baseline w/o RP. Surprisingly, while adding a simple user profile can enhance Personalization, a desired factor aligning with [D2](#page-6-0), Baseline w/o R (i.e., Multimodal LLM + Profile) did not significantly improve resultslike Overall Perceived Scores compared to Baseline w/o RP (i.e., Naive Multimodal LLM), indicating that merely adding a basic user profile to a naive MLLM pipeline is insufficient to generate desired knowledge. This is because the naive MLLM pipeline with personalization produced irrelevant knowledge by overemphasizing "user interests" without considering the user's intentions during primary tasks. As shown in Figure [4,](#page-12-0) when P7, who indicated an interest in "Japanese culture/language" and "alcohol" in their user profile, focused on Japanese alcohol, Baseline w/o R introduced "Japanese drinking etiquette" instead of providing information about the alcohol itself, which was less satisfying than Baseline w/o RP's basic but relevant alcohol category information.

This underscores the need for AI systems to dynamically infer user interests and intentions from real-time context, as users may not explicitly define all their fine-grained desires in advance. It also highlights the importance of coordinating rules with user profiles.

Impact of Knowledge Generation Rules with User Profile: Comparing AiGet vs. Baseline w/o R. AiGet, combining knowledge generation rules with user profiles, provided significantly more desirable knowledge than Baseline w/o R (i.e., Multimodal LLM + Profile) by balancing personalization, context relevance, and alignment with the user's in-situ intentions, highlighting the importance of [D3](#page-6-0). In the example shown in Figure [4,](#page-12-0) AiGet received the highest scores by correctly predicting the user's intention as "deciding which alcohol to purchase, possibly comparing brands," and identifying the user's Focused mode. Following the formative study's rules (Sec [4.5.3\)](#page-6-0) aligning with design goals ([D3](#page-6-0)), it provided balanced information between user interest (e.g., the alcohol name's meaning in Japanese) and usefulness (e.g., comparative flavor profiles with nearby related options).

Additionally, AiGet leveraged gaze pattern analysis to introduce unexpected knowledge about unseen objects at appropriate moments, increasing its utility. For instance, when analyzing P9's Saccade pattern during a casual walk, it introduced "surprising" information about nearby but "unseen" white chickens. In contrast, when P5 Focused on skincare products, AiGet delivered relevant product knowledge, while the baselines introduced irrelevant details (e.g., an air conditioner on the ceiling). Furthermore, AiGet's scoring/weighting and filtering mechanism helped avoid user annoyance by withholding undesired content in lessinteresting indoor areas for two moments.

This ability to balance personalization, usefulness, unexpectedness, and the timely withholding of uninterested content demonstrates AiGet's capability in tailoring information delivery—a capability the baseline systems could not achieve. Comparing all results, this highlights that while [D3](#page-6-0) contributes more to overall knowledge desirability than [D2](#page-6-0), all [D1](#page-6-0)-[D3](#page-6-0) guidelines are still compulsory.

#### 6 Using AiGet in Real-World Scenarios

In-lab evaluations demonstrated that the AiGet pipeline generated knowledge with high levels of novelty, personalization, surprise, and usefulness. While these findings emphasize the system's potential to produce knowledge about its surroundings, its performance in real-time, real-world scenarios remains untested. Therefore, to assess the feasibility of AiGet in everyday environments, we conducted a user study to address the following question:

- RQ1: How feasible is AiGet in creating learning opportunities across various scenarios?
- RQ2: How does AiGet influence users' behaviors and daily routines?
- RQ3: How do users perceive the AiGet experience with continued usage?

It is important to note that fully addressing these questions with a high degree of quantitative evidence would require a larger user base and longer periods of daily usage, which was beyond the scope of this study. However, our study sought to gather user reflections that could provide meaningful insights into these research questions.

Additionally, we did not directly compare the two modes of AiGet— AI-initiated proactive knowledge discovery and userinitiated VQA tasks—in a head-to-head manner. Instead of replacing user-initiated queries, our goal was to explore whether AI-initiated

#### CHI '25, April 26–May 01, 2025, Yokohama, Japan Cai et al.

<span id="page-12-0"></span>![](_page_12_Picture_1.jpeg)

<span id="page-12-1"></span>Figure 4: An example of comparative knowledge from three MLLM pipelines. Note: Participants can't see each pipeline's name (in purple).

![](_page_12_Figure_6.jpeg)

Figure 5: Subjective ratings evaluating the desirability of generated knowledge. \* indicates significance of < 0.05, \*\* indicates significance of < 0.01, \*\*\* indicates significance of < 0.001.

interactions could overcome some limitations of user-initiated queries and enhance informal learning about the surrounding environment. Therefore, both modes were available in this study, allowing users to choose freely, following methods from prior research [\[99\]](#page-21-36).

#### 6.1 Participants

The study consisted of two phases. In Phase 1, 18 participants (8 females, 10 males, Age: = 23.9, = 3.7) from the university community engaged in a one-time use of AiGet. Among these participants, six reported using AI tools (e.g., Google Lens, Chat-GPT app) to identify surrounding objects in their daily activities at least three times per week. Eleven participants used such tools less frequently, ranging from once per month to twice per week. The remaining participant rarely used these tools. All participants self-reported a minimum level of professional working fluency in English.

In Phase 2, 6 participants (3 females, 3 males) from Phase 1 were selected for continued usage based on their demographic diversity and availability (see Appendix [D.2](#page-23-1) for details). These participants, representing a range of self-reported engagement with their surroundings—3 with high engagement and 3 with lower engagement—took part in additional experiments involving multiple sessions.

#### 6.2 Apparatus

As depicted in Figure [1](#page-2-0) and detailed in Sec [5.2,](#page-9-3) participants wore smart glasses and a ring mouse as part of the AiGet system. Participants also carried a lightweight backpack to house the computing unit (i.e., the laptop) during sessions.

The experimenter used a mobile phone (iPhone 12) to record the participants' behaviors and provided a hotspot for internet connectivity.

<span id="page-13-0"></span>![](_page_13_Picture_1.jpeg)

Figure 6: Different Usage Scenarios with AiGet.

#### 6.3 Study Design

In Phase 1, 18 participants were randomly assigned to one of three settings: casual walking on campus, shopping, or visiting a local natural history museum, with 6 participants in each setting. These settings were chosen to represent high, medium, and low frequencies of daily activities, respectively, but with potentially increasing levels of information acquisition desire in the same order.

In Phase 2, 6 participants completed at least two additional sessions overthe following 7 days,resulting in a minimum of 3 sessions per participant. This aligns with prior research on using wearable devices for informal learning in daily routines [\[94\]](#page-21-37). Although participants could not take the device home due to the limited availability of only one prototype, they were able to schedule sessions at any preferred time and location with the experimenters. This led to the exploration of various scenarios, including dining at the canteen, window shopping at a mall, purchasing furniture at IKEA, and visiting art exhibitions, as shown in Figure [6.](#page-13-0) To assess the system's continued/repeated usage in the same places, participants were required to revisit at least one location.

In total, 18 sessions were completed in Phase 1, and 6 out of the 18 participants conducted an additional 22 sessions (P1: 4, P2: 6, P3: 6, P4: 2, P5: 2, P6: 2 sessions) in Phase 2, resulting in 40 sessions in total.

### 6.4 Procedure

In Phase 1, participants signed a consent form, received training, and used the system for at least 45 minutes at designated places. In Phase 2, participants were encouraged to use the system for at least 30 minutes per session at their desired locations. After each session in both Phase 1 and Phase 2, participants completed a questionnaire and participated in a 15-minute interview to share their experiences.

#### 6.5 Measures

To evaluate the feasibility of AiGet forin-the-wild informal learning, we gathered data on the following key measures: 1) the desirability of the knowledge provided by the system, 2) the usability of the system while engaged in primary tasks, and 3) the system's interference with participants' ongoing primary tasks. For details, see Appendix [D.1.](#page-23-0)

6.5.1 Desirability of Knowledge Acquisition. To assess the desirability of knowledge provided by AiGet, we employed both subjective and objective measures. Subjective measures included all those from the in-lab evaluation (Sec [5.3.4\)](#page-11-0), along with a new measure, Increased Environment Connection (7-point Likert scale), to determine whether AiGet enhances users' connection with their surrounding environment.

Objective measures included tracking the count of AI-initiated knowledge, the frequency of canceled displays, user-initiated queries [\[25\]](#page-19-15), and the immediate recall of novel knowledge. Additionally, we recorded instances where participants perceived the system as providing incorrect information.

6.5.2 Usability of the System with Primary Tasks. We measured usability using the System Usability Scale (SUS) [\[19\]](#page-19-23) and assessed perceived cognitive load through the RTLX scale [\[42\]](#page-20-36), providing insights into the effort required to use AiGet while managing primary tasks.

6.5.3 Interference with Primary Tasks. Following prior research on wearable assistants and interactions with smart glasses during daily activities [\[25,](#page-19-15) [26,](#page-19-22) [49\]](#page-20-34), we assessed participants' experiences with their primary tasks, including Distraction, Enjoyment of Primary Task, and Naturalness, using 7-point Likert scales.

#### 6.6 Findings

Table [3](#page-15-0) and Figure [7](#page-14-1) present the descriptive statistics for all measures. Following the same analysis procedure as in the formative study (Sec [4.4\)](#page-5-5), we address our research questions.

<span id="page-13-1"></span>6.6.1 RQ1: How feasible is AiGet in creating learning opportunities across various scenarios? Before conducting the user study, we had several concerns about how users might perceive AiGet. These concerns were focused on three main points: (1) uncertainty about the quality and relevance of AI-generated knowledge in real-world testing, (2) whether information delivered proactively would disrupt or annoy users during their daily activities, and (3) doubts about how valuable users would find information they didn't ask for. These concerns reflected broader questions about how to effectively integrate AI-driven informal learning into daily life. We were particularly interested in seeing how users would react to and benefit from this new approach to learning and information delivery.

The results were notably positive, surpassing our initial expectations and alleviating our concerns. Participants spent an average of 43.0 minutes (SD = 13.6) per session using AiGet. Addressing our first concern, they found AiGet provided useful (5.9 out of 7, on average), interesting (6.0 out of 7), surprisingly unexpected (6.0 out of 7), and relevant (6.1 out of 7) knowledge, deepening their understanding on the topics (6.1 out of 7). Contrary to our second concern, while proactively prompting users with 1.26 knowledge per minute on average, AiGet did not cause significant distraction (2.2 out of 7) to users' primary tasks. Resolving our third concern, it helped all participants gain "unseen and unknown knowledge" in various daily activities, making them feel like they had a "friend (P4)" or "companion (P2, P6, P11)", and fostered a deep connection with the environment (6.0 out of 7). Additionally, participants perceived AiGet as having 'Excellent' usability (SUS: 88.4, higher

#### <span id="page-14-1"></span>CHI '25, April 26–May 01, 2025, Yokohama, Japan Cai et al.

![](_page_14_Figure_1.jpeg)

Figure 7: Subjective ratings on the desirability of generated knowledge, its impact on primary tasks, and system usability in different scenarios.

than 80 [\[10\]](#page-19-24)), while incurring a low cognitive load (RTLX: 16.6) and perceiving natural (6.0 out of 7) to use during primary tasks, further supporting its successful integration into everyday life.

As expected, AiGet's proactive AI-initiation generated significantly more knowledge (M=54.2) compared to passive Userinitiation (M=2.3). Notably, despite this substantial increase in knowledge delivery, AI-initiation had a surprisingly low cancellation rate (2.4/54.2 ≈ 4%), indicating that users found proactive knowledge delivery acceptable most of the time. Additionally, users recalled an average of 7.8 novel knowledge instances per session (AI-initiation: M=7.1, User-initiation: M=0.7), highlighting AiGet's knowledge acquisition support.

Together, these findings underscore AiGet's ability to help users acquire more knowledge than through self-directed queries alone, overcoming barriers like "I don't know what to ask (P16)". Furthermore, participants acknowledged that both AI-initiated and user-initiated interactions play "supplementary roles (P7)". For instance, P4 shared that "initially, I have no question as I don't know what it [flying squirrel] is until it [AiGet] shares information with me, so I asked a follow-up question with this inspiration." This demonstrates how proactive 'desirable' knowledge delivery can spark curiosity and encourage further exploration by users.

<span id="page-14-2"></span>Usage in Different Scenarios. As shown in Figure [7,](#page-14-1) AiGet provides an equally good experience in acquiring knowledge with enhanced enjoyment for primary tasks in different scenarios. Interestingly, our study revealed that users perceived AiGet in diverse roles across different contexts, despite its consistent knowledgegeneration mechanism. These roles emerged naturally, shaped by users' expectations, goals, and activity contexts. This highlights how users' mindsets and situational needs influence their interpretation of AiGet, affecting how they integrate and value the information in various aspects during daily activities.

(1) During Casual Walking in Familiar Places, users perceived AiGet as "Environmental Storyteller" to introduce "Familiar Strangers" in their environment. This included knowledge that interested users but was unexplored due to time constraints (i.e., known unknowns) and knowledge users hadn't anticipated (i.e., unknown unknowns) about the environment. For example, P2

appreciated when AiGet detected the school logo and explained the meaning behind its colors, "I have a design background and knew the general color meaning, but I had never connected them to the school logo until AiGet told me. It was a pleasant surprise that strengthened my connection to the school." Additionally, as demonstrated in Figure [8,](#page-14-0) P9 was amazed to learn that two concrete walls on campus were actually Berlin Wall, "I frequently passed by but never expected to find pieces of the real Berlin Wall on our campus." This type of discovery was particularly well-received during casual walks, where users were in a more relaxed mindset, making them more receptive to narrative-like information about their surroundings.

<span id="page-14-0"></span>![](_page_14_Picture_9.jpeg)

Figure 8: An example of AiGet helping users identify 'unknown unknowns' in daily life: two walls on campus are part of the Berlin Wall, with accompanying information introducing its history.

(2) During Shopping & Dining, AiGet was seen as a "Personalized Decision-Making Supporter." It helped mitigate uncertainty avoidance [\[75\]](#page-21-4) and facilitate informed choices by providing customized introductions, breaking down language barriers, and aligning suggestions with personal preferences. For example, P1 and P4 had previously avoided imported beverages and food with

<span id="page-15-0"></span>

| Scenario (Count)             | Duration (min)      | AI-Initiated Knowledge | Cancel Display    | Immediate Recall  | User-Initiated Query | Perceived Wrong   |
|------------------------------|---------------------|------------------------|-------------------|-------------------|----------------------|-------------------|
| Casual Walking (10)          | 𝑀 = 43.6, 𝑆𝐷 = 17.6 | 𝑀 = 51.1, 𝑆𝐷 = 24.8    | 𝑀 = 2.8, 𝑆𝐷 = 2.6 | 𝑀 = 6.8, 𝑆𝐷 = 3.9 | 𝑀 = 1.7, 𝑆𝐷 = 2.5    | 𝑀 = 0.1, 𝑆𝐷 = 0.3 |
| Museum & Art Exhibition (13) | 𝑀 = 46.8, 𝑆𝐷 = 13.5 | 𝑀 = 62.4, 𝑆𝐷 = 20.0    | 𝑀 = 2.4, 𝑆𝐷 = 2.8 | 𝑀 = 7.9, 𝑆𝐷 = 5.6 | 𝑀 = 3.0, 𝑆𝐷 = 2.5    | 𝑀 = 1.6, 𝑆𝐷 = 1.0 |
| Shopping & Dining (17)       | 𝑀 = 39.6, 𝑆𝐷 = 10.8 | 𝑀 = 49.9, 𝑆𝐷 = 15.7    | 𝑀 = 1.9, 𝑆𝐷 = 3.3 | 𝑀 = 8.2, 𝑆𝐷 = 5.1 | 𝑀 = 2.4, 𝑆𝐷 = 3.0    | 𝑀 = 0.8, 𝑆𝐷 = 0.9 |
| All Sessions (40)            | 𝑀 = 43.0, 𝑆𝐷 = 13.6 | 𝑀 = 54.2, 𝑆𝐷 = 20.0    | 𝑀 = 2.4, 𝑆𝐷 = 2.9 | 𝑀 = 7.8, 𝑆𝐷 = 4.9 | 𝑀 = 2.3, 𝑆𝐷 = 2.9    | 𝑀 = 0.8, 𝑆𝐷 = 1.0 |
|                              |                     |                        |                   |                   |                      |                   |

Table 3: Objective descriptive statistics for AiGet system usage in different scenarios.

foreign language labels due to unfamiliarity and language barriers. However, with AiGet, P1 found he could easily "navigate around and understand what I'm looking at," ultimately selecting a suitable drink he had never tried before. Similarly, P2 ordered a dish he had never tried until AiGet introduced its unique cooking method and taste, aligning with P2's preferences. Furthermore, AiGet helped P6 make quick choices and notice unseen but useful options, aligning with her health values: "It saved me time by telling me which bean was lower in calories, comparing them with similar ones that I hadn't noticed on the same shelf." Thus, these highlighted benefits of perceived role align with users' primary goal in these contexts: making decisions.

(3) In Museum & Art Exhibitions, AiGet's role was perceived as a "Personalized Information Highlighter". It enhanced the educational experience by assisting users in extracting key information from exhibition boards, helped them notice themes or elements they might have overlooked, and provided additional facts aligned with their personal interests. P3, for example, often scans museum boards quickly, missing certain details. AiGet, aware of her design background, highlighted how metal elements affect ceramic colors, which she initially overlooked. It also provided surprising information about the Persian origin of cobalt blue dyes, expanding her knowledge. Similarly, P14 appreciated how AiGet highlighted the constant replacement of shark teeth, something she hadn't noticed during her initial scan, enhancing her learning in a concise and efficient manner. This perception likely arises from users' goal of absorbing information efficiently in potentially overwhelming educational settings.

These cases revealed that, despite using the same information generation mechanism, AiGet becomes naturally useful in diverse contexts by tailoring information to a person's preferences and background. This adaptability highlights the potential of AI systems to seamlessly integrate into various aspects of daily life. It suggests that well-designed AI assistants can enhance user experiences across different scenarios without altering their core mechanisms. Instead, their perceived value and functionality evolve based on the user's needs and context, opening new possibilities for designing versatile AI systems that fluidly adapt to changing circumstances and goals.

#### <span id="page-15-1"></span>6.6.2 RQ2: How does AiGet influence users' behaviors and daily routines?

While our initial goal for AiGet was modest—helping users become aware of initial ignored yet desirable knowledge from everyday moments—the study results exceeded our expectations. We discovered that AiGet is more than just a tool for displaying hidden

knowledge; it shows potential as a medium that shapes user behavior and transforms daily routines, drawing users' "attention back to their in-situ environment (P1)". This shift has three main effects.

(1) Most (16/18) users reported it made them "more observant and increased their curiosity about their surroundings (P13)". This shift addresses users' previous "short attention span due to smartphone usage (P1)" and task-driven lifestyle, which often led to behaviors such as "rushing during commuting (P2)", "skimming in museums (P16)", and "shopping with only specific items in mind (P17)". Now, users are more interested in their surrounding environment and willing to explore further, as they can obtain useful and surprising knowledge from the system. By "fostering curiosity (P15)" and "deeper engagement (P7)", AiGet helps users shift from passive interaction with their surroundings to a more exploratory mindset, unlocking "hidden layers [e.g., perspectives, benefits]" of their everyday environments.

(2) One such unlocked "hidden layers" brought by prolonged attention is to revive suspended personal interests or hidden wishes. For instance, P8, a biology major with an interest in typography, was surprised when AiGet provided an unexpected perspective: "I usually enjoy looking at posters and notices, and AiGet surprised me with information about font design [Helvetica] on a board. It was quite pleasant. (P8)" Similarly, P15, who had a long-term hidden interest in car models, now enjoys walking by the road with AiGet, gaining deeper knowledge about each model's characteristics. This demonstrates how AI systems like AiGet can bring unexpected joy and discovery, aligning with personal interests into routine activities, rekindling forgotten passions, and creating richer experiences in users' daily lives.

(3) Building on these benefits, participants noted AiGet's potential to reduce reliance on smartphones for social media in some situations, a primary method for informal learning as mentioned in the formative study (Sec [4.5.1\)](#page-5-3). While social media offers "in-depth information on trending topics (P14)" and is "suitable for use at home (P6)", participants highlighted AiGet's ability to replace social media during "everyday activities (P6)", providing a "broader understanding of the world (P14)". Three participants reported less phone usage while queuing for checkout in stores: "Previously, I would check social media to pass time. Now, I was observing my surroundings and checking products because I could gain interesting information related to my current environment. (P2)" AiGet was also perceived as more memorable and connected to the environment than social media. P9 commented: "It gives me new information about the environment that I will remember and explore further, but information from my phone is often forgotten after I see it." P15 added: "If I'm scrolling through TikTok, I feel it's less useful. But this [AiGet] helps you understand your environment, which could be useful later on."

The above cases indicate that, while changing habits in daily routines is typically challenging, AiGet offers a unique approach by delivering engaging and valuable information that enables users to naturally and effortlessly adjust their behaviors. This points to the potential for future AI assistants to subtly and positively influence user habits, making behavior change seamless and enjoyable through meaningful experiences.

6.6.3 RQ3: How do users perceive the AiGet experience with continued usage? While our study was conducted up to 7 sessions per participant across different days, insufficient for long-term usage analysis, we gathered initial feedback on continued usage in various locations. As illustrated in Figure [9,](#page-17-1) the positive experience of consistently gaining desired knowledge, with minimal interference to primary tasks, did not diminish after continued usage. Instead, it demonstrated an increasing trend. We further examined user feedback after multiple usage in different or the same places.

Usage in Different Places (16 out of 22 additional usages). Most (5/6) participants appreciated usage at different places as they provided different values, as also demonstrated in RQ1 (Sec [6.6.1\)](#page-14-2). Interestingly, over time, participants developed personalized usage habits, leveraging the system's flexible output options to match their knowledge acquisition needs and mindsets in various scenarios. For example, while shopping, P4 preferred to mute the system and receive text-only feedback for quick, information-dense updates, which enables him to quickly "browse various products in a short time and glance at the information only when needed". But during casual walks, he opted for audio feedback to relax.

However, one participant, P5, preferred limiting the system's use to specific contexts (e.g., shopping). Valuing "efficiency of acquiring information in life", she found that knowledge that is merely "good to have" (e.g., surprise by the bird in the environment) wasn't worth the cognitive effort if it lacked immediate usefulness.

These cases emphasize the importance of designing proactive AI systems with flexible output options and on/off controls, catering to different user preferences and the varying demands of long-term usage in diverse scenarios.

Usage in Same Places (6 out of 22 additional usages). With a design to filter similar knowledge and provide alternative perspectives of the same entities (Sec [5.1.3\)](#page-7-4), repeated exposure to the same place did not reduce novel knowledge intake (First Time vs. Second Time Novelty: 5.7 vs. 5.8 out of 7) as long as the environment was "dynamic and large (P6)". As P1 described it, "it's like hunting new things that I never found. (P1)"

Interestingly, because participants could become more observant, as mentioned in Sec [6.6.2,](#page-15-1) it helped them find new interests in familiar environments, even when the AiGet system was not in use. P2, who used AiGet for 7 days, mentioned that before using it, he usually didn't pay attention to his familiar environment during his commute. However, after using the system (and twice in commuting), he became more observant, even without AiGet, and noticed a Hornbill building a nest one morning by a school building, wishing he had the system to learn more about Hornbills.

Although long-term usage still needs further investigation, these use cases highlight the feasibility and utility of proactive AI assistants for informal learning.

#### 7 Overall Discussion

The AiGet system demonstrates a potential for transforming everyday moments into informal learning opportunities by effectively addressing longstanding barriers to learning from the in-situ environment. By proactively analyzing user attention and context, AiGet uncovers knowledge often overlooked, bridging the gap from "unseen" to "seen", from "seen" to "known", and from "known" to "known better". This minimized-effort approach deepens user engagement with their surroundings, enhancing participants' curiosity and observational skills while reviving hidden interests and fostering a willingness to explore both familiar and unfamiliar environments.

In the following discussion, we will highlight two key aspects: 1) how wearable knowledge discovery assistants (i.e., wearable AI assistants supporting knowledge discovery) can be used for informal learning in daily life, and 2) what improvements are needed for their long-term usage. By examining these aspects, we aim to provide insights into the future development and integration of context-aware wearable knowledge discovery assistants in everyday contexts, further improving how we acquire and interact with information from our in-situ environment.

### 7.1 How Wearable Knowledge Discovery Assistants Can Be Used for Informal Learning in Daily Life?

<span id="page-16-0"></span>7.1.1 Effective Components for Addressing Cognitive Limitations and Biases. As shown in the real-world study (Sec [6.6.1\)](#page-13-1), the AiGet system demonstrates the ability to reduce cognitive limitations and biases associated with informal learning through the following three key components.

Proactivity with Context Awareness. This feature addresses "Known Unknowns" arising from time constraints, attentional limitations, and uncertainty avoidance [\[11,](#page-19-3) [75\]](#page-21-4) (Sec [4.5.1\)](#page-5-3) by proactively delivering context-relevant information with reduced manual effort. This approach has encouraged users to revive hidden interests and leave their comfort zones, leading to "increased interaction with unfamiliar environments (P4)". Moreover, such proactivity helps correct "wrong assumptions" users often make when encountering unfamiliar entities (Sec [4.5.1\)](#page-5-3). For example, in the real-world study, P2 saw a vending machine on campus and assumed it was selling ice cream based on the packaging and refrigeration. AiGet informed him it was actually selling protein bars, surprising P2 and making him remember the machine for "future use before workouts".

Gaze Pattern Analysis. By analyzing user gaze patterns and identifying information in both gazed and ignored (i.e., not gazed) areas of the in-situ environment, AiGet addresses inattentional blindness [\[69\]](#page-21-3) (Sec [4.5.1\)](#page-5-3). This helps identify unseen yet interesting or useful information that users might otherwise overlook.

Extensive Knowledge Access with Personalization. By utilizing the vast knowledge within LLMs (derived from training data) and tailoring it to user familiarity through personalized profiles, AiGet presents "Unknown Unknown" perspectives, often surprising users. This addresses the Illusion of Explanatory Depth [\[84\]](#page-21-5) (Sec [4.5.1\)](#page-5-3) and deepens users' understanding of their surroundings, providing unexpected insights, especially for seemingly familiar objects or concepts.

<span id="page-17-1"></span>![](_page_17_Figure_1.jpeg)

Figure 9: Subjective ratings on the desirability of generated knowledge, its impact on primary tasks, and system usability on different days (up to 7 days).

Each of these components addresses distinct cognitive limitations and biases, highlighting the value of integrated systems like AiGet in enhancing informal learning opportunities in everyday life.

7.1.2 Potential Caveats of AI Biases and Design Considerations. Although utilizing AI, especially LLM's extensive knowledge base, shows promise, careful design is crucial to avoid introducing AI biases. This is particularly important when presenting information about entities or topics with both positive and negative aspects. For instance, full-fat milk can be interpreted as either 'nutritious due to its vitamins and minerals' or as 'high in fat content'. For a health-conscious user, deciding which perspective to prioritize is crucial. To address this challenge, the AI needs to understand more nuanced user preferences and contexts. For instance, if a user is ill, the system could emphasize the nutritional benefits of full-fat milk, whereas for a user focused on weight management, it could highlight its high-calorie content. This fine-tuning ensures the information aligns with the user's situational needs.

Additionally, the AI can present multiple perspectives over time, alternating between positive and negative aspects to provide a wellrounded understanding of complex topics. This approach helps prevent biased or one-sided information while tailoring the experience to users' evolving needs.

7.1.3 Improving User Trust for "Surprising" Content. Enhancing user trust in wearable knowledge discovery assistants is crucial, particularly when the system presents surprising or counterintuitive information. As P1 noted, prior negative experiences with AI-generated content (e.g., hallucinations [\[9\]](#page-19-25)) can cause users to distrust unexpected information, even when it is accurate—such as learning that 'a palm tree is more closely related to grass than typical trees'.

To address this issue, future systems should incorporate several key features. First, a more refined scoring/prioritization system is needed. Rather than relying on the binary assessments used in the current AiGet prototype (Sec [5.1.3\)](#page-7-4), future versions could implement a nuanced scoring system to measure "unexpectedness." When this score is high, the system could provide reference marks

to enhance credibility, similar to practices in some existing AI systems (e.g., [perplexity.ai\)](https://www.perplexity.ai/). Additionally, a "check source" button could be introduced, allowing users to verify detailed and authentic explanations as needed. This design of offering references only when necessary enhancestrust without overwhelming users, maintaining a seamless experience for consuming information on the go.

### <span id="page-17-0"></span>7.2 Improvements for Long-Term Usage of Context-Aware Wearable Knowledge Discovery Assistants

7.2.1 Balancing Knowledge Diversity and Depth. In the real-world study, participants highlighted AiGet's ability to "break out of information cocoons [exposed only to information that aligns with their existing beliefs or interests] (P3)" by providing information on both prior interested topics and relatively unfamiliar content embedded in the environment, thus expanding their "knowledge base (P18)". While this feature fosters the discovery of new interests, it also raises concerns about potential information overload. As P1 noted, "I don't need to be an expert on everything," emphasizing the need for careful design in content delivery, especially for repeated exposure to entities of limited interest.

To enhance long-term engagement with wearable knowledge discovery assistants, we propose a dynamic approach that balances knowledge diversity and depth, adjusting to user interests and environmental changes over time. Based on the knowledge participants gained in our studies, we categorize knowledge depth into three levels, aligned with the progression from "seen" to "known" to "known well": (Level 1) Alerting users to the existence of unseen, new, or changed entities in the environment. (Level 2) Providing surfacelevel yet interesting or surprising information about these "seen" entities. (Level 3) Delivering detailed or expert-level information tailored to users' specific interests.

For repetitive use in relatively familiar environments, the system should prioritize Levels 1 and 2 when users are Saccade-ing or Quick Browse-ing (Sec [4.5.2\)](#page-5-4). This approach helps users discover accessible knowledge and build broader connections with the environment. Level 3 should be reserved fortopics where users have demonstrated sustained interest (e.g., focusing on something), fostering deeper

connections with subjects they find interesting. This tiered, contextadaptive approach balances breadth and depth, mitigating overload while personalizing the learning experience.

7.2.2 Enhancing Coherence Between Learning Moments. Participants suggested that future systems could strengthen learning by linking new information to previous experiences, supporting scaffolding [\[97\]](#page-21-38) to build structured knowledge for long-term retention and creating more memorable interactions. This idea aligns with research on LLM companions [\[104\]](#page-21-20), which use historical context to enhance user engagement and comprehension, and wearable memory assistants like Memoro [\[112\]](#page-22-14), which use past context to augment user memory.

For example, two participants mentioned that AiGet's knowledge prompts reminded them of their undergraduate studies, making the information more memorable. However, during P14's museum visit, AiGet initially provided details on limb evolution from fins but missed the opportunity to connect this [10](#page-18-0) information to a Dugong specimen with limb-like features later in the visit. This oversight disappointed the participant and highlighted the need for future systems to automatically link related information across different moments. To address this, future systems could use techniques like knowledge graphs or Retrieval-Augmented Generation (RAG) [\[109\]](#page-22-11) to identify and connect relevant knowledge across various encounters.

7.2.3 Considering Interruptibility in Real-Time Mode and Supporting Retrospective Mode. To reduce the interference to primary tasks, beyond providing easy-to-digest content and enabling manual output adjustment, the future system could further predict user interruptibility [\[79\]](#page-21-39) and automatically reduce its frequency or turn off during moments when users prefer fewer interruptions (as shown in Figure [2-](#page-8-0)Gray parts). This feature can be implemented by 1) Using MLLMs for FPV analysis and withholding notifications when users are engaged in high-concentration or time-sensitive activities (e.g., crossing a road, focusing on work, and rushing in train stations); 2) Incorporating additional input modalities, such as Electrodermal Activity signals [\[93\]](#page-21-40), to discern variations in mental or emotional states even within the same activity, allowing nuanced adjustments (e.g., distinguishing whether a student is thinking of homework after class or enjoying a leisurely walk home); and 3) Adapting interruptibility rules based on individual preferences and interaction history.

Complementing this real-time adaptation, participants (e.g., P3) suggested incorporating a retrospective mode to catch up on interesting or useful information they might have missed while the system is in "Do Not Disturb" status. This retrospective mode could also encourage reflection on daily experiences, offering meta-cognitive insights excluded in real-time mode due to the high cognitive load involved. Additionally, it could converse with users during this period and dynamically update user preferences in the profile based on their reflections on interactions throughout the day.

7.2.4 Privacy, Security, and Ethical Considerations. As noted in prior research on wearable intelligent assistants [\[25,](#page-19-15) [51\]](#page-20-23), privacy, security, and ethical concerns are critical for both users and bystanders [\[1\]](#page-19-26). Key issues include continuously monitoring real-time data (e.g., gaze patterns and environmental interactions), user preferences, and context data without explicit consent from bystanders. To address these concerns, AiGet ensures that real-time gaze and context data is not stored and provides flexible on/off settings to prevent FPV capture in sensitive environments. Additionally, using Gemini Models' safety setting[s11](#page-18-1) helps filter inappropriate content and reduce the risk of harassment to others.

For future systems, leveraging local MLLMs like LLaVA [\[65\]](#page-21-41) could enhance privacy and security by reducing reliance on cloud services, thereby limiting exposure of private data. User profiles should also be encrypted when stored locally to safeguard personal information. From an ethical standpoint, as discussed in Sec [7.1.1,](#page-16-0) it is crucial for future systems to avoid bias in learning content and ensure fairness in knowledge delivery to all users.

#### 8 Limitations

Study Limitations. While our evaluation provided initial insights into using a proactive wearable knowledge discovery assistant, it had several limitations that future research should address. First, our study focused on low-demand daily activities, such as casual walking and shopping. It remains unclear whether the designed mitigations for reducing task disruption will be as effective in higherdemand scenarios, such as time-sensitive tasks. Future research should explore how proactive knowledge discovery assistants perform under varying cognitive and temporal constraints. Additionally, the study primarily involved university students and staff, who may represent early adopters of smart glasses technology. Future studies should include a more diverse demographic to ensure broader applicability. While we conducted multi-session studies over a week, longer-term evaluations are necessary to better understand the system's impact on learning behaviors and retention. Moreover, due to hardware constraints, participants could not use the system continuously in their daily lives without being observed, potentially affecting the naturalness of theirinteractions. To address these gaps, future research should conduct extended, real-world deployments to capture more authentic usage patterns and assess long-term effects on learning and behavior.

System Limitations. While our proof-of-concept wearable AI assistant for informal learning shows promise, it faces several challenges that must be addressed in future iterations. A well-known issue is the occurrence of inaccuracies and hallucinations in LLMgenerated content [\[9\]](#page-19-25). To improve accuracy and reliability, future versions should incorporate reliable sources for cross-referencing and implement Retrieval-Augmented Generation (RAG) [\[109\]](#page-22-11) with a curated knowledge base. Additionally, the current processing time of 8–12 seconds can be disruptive in information-rich environments (e.g., museums), especially if users wait for system output during informal learning. While speed improvements are expected with more advanced, lightweight LLM models, participants suggested that brief delays (e.g., 5 seconds) could be beneficial. This pause allows users to observe entities independently before receiving AIgenerated insights, particularly during focused observation of their

<span id="page-18-0"></span><sup>10</sup>The dugong is a highly endangered marine mammal, which is commonly known as 'sea cows'.

<span id="page-18-1"></span><sup>1</sup>[1https://ai.google.dev/gemini-api/docs/safety-settings](https://ai.google.dev/gemini-api/docs/safety-settings)

surroundings. Therefore, such wearable assistants should support adaptive delay mechanisms that respond to the user's attention level, intentions, and environmental context.

#### 9 Conclusion

We explored the feasibility of a wearable AI assistant, AiGet, that proactively analyzes users' in-situ environments to provide personalized, context-relevant, yet often overlooked knowledge with minimal disruption to primary tasks. Through in-lab and real-world evaluations, we demonstrated AiGet's utility in uncovering overlooked interests, enhancing task enjoyment, reviving curiosity, and deepening connections with the environment. Our findings highlight the potential of AI-assisted informal learning to transform everyday moments into enriching learning experiences. We also identified key design considerations for future systems, including balancing knowledge diversity and depth, enhancing coherence between learning moments, and supporting real-time and retrospective interaction modes. We have open-sourced this project at: <https://github.com/Synteraction-Lab/AiGet> and welcome contributions from the community to enhance its capability. As wearable AI assistants continue to evolve, AiGet represents a promising initial step toward seamlessly integrating informal learning into daily life, opening new avenues for ubiquitous knowledge discovery and personal growth.

#### Acknowledgments

This research is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (Award Number: AISG2-RP-2020-016). The CityU Startup Grant (9610677) also provides partial support. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore. We extend our gratitude to the Lee Kong Chian Natural History Museum and NUS Museum for their invaluable assistance with our user studies and to all members of the Synteraction Lab for their help in completing this project. We also thank the reviewers for their valuable feedback.

#### References

- <span id="page-19-26"></span>[1] Fouad Alallah, Ali Neshati, Yumiko Sakamoto, Khalad Hasan, Edward Lank, Andrea Bunt, and Pourang Irani. 2018. Performer vs. observer: whose comfort level should we consider when examining the social acceptability of input modalities for head-worn display?. In Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology (VRST '18). Association for Computing Machinery, New York, NY, USA, 1–9. <https://doi.org/10.1145/3281505.3281541>
- <span id="page-19-11"></span>[2] Hashem A. Almusawi, Christopher M. Durugbo, and Afaf M. Bugawa. 2021. Wearable Technology in Education: A Systematic Review. IEEE Transactions on Learning Technologies 14, 4 (Aug. 2021), 540–554. [https://doi.org/10.1109/TLT.](https://doi.org/10.1109/TLT.2021.3107459) [2021.3107459](https://doi.org/10.1109/TLT.2021.3107459) Conference Name: IEEE Transactions on Learning Technologies.
- <span id="page-19-21"></span>[3] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human-AI Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19). Association for Computing Machinery, New York, NY, USA, 1–13. <https://doi.org/10.1145/3290605.3300233>
- <span id="page-19-9"></span>[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).
- <span id="page-19-2"></span>[5] Riku Arakawa, Hiromu Yakura, and Sosuke Kobayashi. 2022. VocabEncounter: NMT-powered Vocabulary Learning by Presenting Computer-Generated Usages of Foreign Words into Users' Daily Lives. In CHI Conference on Human Factors in Computing Systems. ACM, New Orleans LA USA, 1–21. [https://doi.org/10.](https://doi.org/10.1145/3491102.3501839) [1145/3491102.3501839](https://doi.org/10.1145/3491102.3501839)
- <span id="page-19-0"></span>[6] Nicole M Ardoin and Joe E Heimlich. 2021. Environmental learning in everyday life: foundations of meaning and a context for change. Environmental Education Research 27, 12 (2021), 1681–1699.
- <span id="page-19-4"></span>[7] Edited Jill Attewell and Carol Savill-Smith. 2004. Mobile learning anytime everywhere. Mobile learning (2004).
- <span id="page-19-12"></span>[8] Ronald T Azuma. 1997. A Survey of Augmented Reality. Presence: Teleoperators and Virtual Environments 6, 4 (Aug. 1997), 355–385. [https://doi.org/10.1162/](https://doi.org/10.1162/pres.1997.6.4.355) [pres.1997.6.4.355](https://doi.org/10.1162/pres.1997.6.4.355)
- <span id="page-19-25"></span>[9] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023).
- <span id="page-19-24"></span>[10] Aaron Bangor, Philip T. Kortum, and James T. Miller. 2008. An Empirical Evaluation of the System Usability Scale. International Journal of Human–Computer Interaction 24, 6 (July 2008), 574–594. <https://doi.org/10.1080/10447310802205776>
- <span id="page-19-3"></span>[11] Jonathan Baron. 2023. Thinking and deciding. Cambridge University Press.
- <span id="page-19-6"></span>[12] Jennifer S. Beaudin, Stephen S. Intille, Emmanuel Munguia Tapia, Randy Rockinson, and Margaret E. Morris. 2007. Context-Sensitive Microlearning of Foreign Language Vocabulary on a Mobile Device. In Ambient Intelligence, David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Bernt Schiele, Anind K. Dey, Hans Gellersen, Boris de Ruyter, Manfred Tscheligi, Reiner Wichert, Emile Aarts, and Alejandro Buchmann (Eds.). Vol. 4794. Springer Berlin Heidelberg, Berlin, Heidelberg. [https://doi.org/10.1007/978-3-540-76652-0\\_4](https://doi.org/10.1007/978-3-540-76652-0_4)
- <span id="page-19-1"></span>[13] Zvi Bekerman, Nicholas C Burbules, and Diana Silberman-Keller. 2006. Learning in places: The informal education reader. Vol. 249. Peter Lang.
- <span id="page-19-13"></span>[14] Mark Billinghurst, Adrian Clark, and Gun Lee. 2015. A Survey of Augmented Reality. Foundations and Trends® in Human–Computer Interaction 8, 2-3 (March 2015), 73–272. <https://doi.org/10.1561/1100000049>
- <span id="page-19-17"></span>[15] M. Blum, A. Pentland, and G. Troster. 2006. InSense: Interest-Based Life Logging. IEEE MultiMedia 13, 4 (2006), 40–48. <https://doi.org/10.1109/MMUL.2006.87>
- <span id="page-19-10"></span>[16] Pietro Bongini, Federico Becattini, Andrew D. Bagdanov, and Alberto Del Bimbo. 2020. Visual Question Answering for Cultural Heritage. IOP Conference Series: Materials Science and Engineering 949, 11 (Nov. 2020), 012074. [https://doi.org/](https://doi.org/10.1088/1757-899X/949/1/012074) [10.1088/1757-899X/949/1/012074](https://doi.org/10.1088/1757-899X/949/1/012074)
- <span id="page-19-7"></span>[17] Tayeb Brahimi and Akila Sarirete. 2015. Learning outside the classroom through MOOCs. Computers in Human Behavior 51 (2015), 604–609.
- <span id="page-19-19"></span>[18] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative Research in Psychology 3, 2 (Jan. 2006), 77–101. [https://doi.org/10.](https://doi.org/10.1191/1478088706qp063oa) [1191/1478088706qp063oa](https://doi.org/10.1191/1478088706qp063oa)
- <span id="page-19-23"></span>[19] John Brooke. 1996. SUS - A quick and dirty usability scale. Usability evaluation in industry 189, 194 (1996), 7.
- <span id="page-19-8"></span>[20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. <https://doi.org/10.48550/arXiv.2005.14165>
- <span id="page-19-20"></span>[21] Brian Bruya. 2010. Effortless attention: A new perspective in the cognitive science of attention and action. MIT Press.
- <span id="page-19-18"></span>[22] Daniela Buzova, Amparo Cervera-Taulet, and Silvia Sanz-Blas. 2020. Exploring multisensory place experiences through cruise blog analysis. Psychology & Marketing 37, 1 (2020), 131–140. <https://doi.org/10.1002/mar.21286> arXiv[:https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.21286](https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.21286)
- <span id="page-19-14"></span>[23] Arthur Caetano, Alyssa Lawson, Yimeng Liu, and Misha Sra. 2023. ARLang: An Outdoor Augmented Reality Application for Portuguese Vocabulary Learning. In Proceedings of the 2023 ACM Designing Interactive Systems Conference. ACM, Pittsburgh PA USA, 1224–1235. <https://doi.org/10.1145/3563657.3596090>
- <span id="page-19-5"></span>[24] Carrie J. Cai, Anji Ren, and Robert C. Miller. 2017. WaitSuite: Productive Use of Diverse Waiting Moments. ACM Transactions on Computer-Human Interaction 24, 1 (March 2017), 1–41. <https://doi.org/10.1145/3044534>
- <span id="page-19-15"></span>[25] Runze Cai, Nuwan Janaka, Yang Chen, Lucia Wang, Shengdong Zhao, and Can Liu. 2024. PANDALens: Towards AI-Assisted In-Context Writing on OHMD During Travels. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 1053, 24 pages. [https:](https://doi.org/10.1145/3613904.3642320) [//doi.org/10.1145/3613904.3642320](https://doi.org/10.1145/3613904.3642320)
- <span id="page-19-22"></span>[26] Runze Cai, Nuwan Janaka, Shengdong Zhao, and Minghui Sun. 2023. Para-GlassMenu: Towards Social-Friendly Subtle Interactions in Conversations. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 721, 21 pages. <https://doi.org/10.1145/3544548.3581065>
- <span id="page-19-16"></span>[27] Ruei-Che Chang, Yuxuan Liu, and Anhong Guo. 2024. WorldScribe: Towards Context-Aware Live Visual Descriptions. In Proceedings of the 37th Annual ACM

Symposium on User Interface Software and Technology (Pittsburgh, PA, USA) (UIST '24). Association for Computing Machinery, New York, NY, USA, Article 140, 18 pages. <https://doi.org/10.1145/3654777.3676375>

- <span id="page-20-18"></span>[28] Jian Chen, Yueqin Zhang, Jingyu Sun, Yongle Chen, Fuping Lin, and Qun Jin. 2015. Personalized micro-learning support based on process mining. In 2015 7th International Conference on Information Technology in Medicine and Education (ITME). IEEE, 511–515.
- <span id="page-20-22"></span>[29] Sharon Lynn Chu, Brittany M. Garcia, and Neha Rani. 2023. Research on wearable technologies for learning: a systematic review. Frontiers in Education 8 (Nov. 2023). <https://doi.org/10.3389/feduc.2023.1270389> Publisher: Frontiers.
- <span id="page-20-21"></span>[30] Randell Cliff. 2005. Wearable Computing: A Review. Wearable Computing: A Review (2005). Publisher: University of Bristol.
- <span id="page-20-8"></span>[31] Thomas J Conlon. 2004. A review of informal learning literature, theory and implications for practice in developing global professional competence. Journal of European industrial training 28, 2/3/4 (2004), 283–295.
- <span id="page-20-1"></span>[32] Jay Cross. 2007. Informal learning: Rediscovering the natural pathways that inspire innovation and performance. Pfeiffer/John Wiley & Sons, San Francisco, CA, US. Pages: xxi, 292.
- <span id="page-20-16"></span>[33] Mustafa Doga Dogan, Eric J. Gonzalez, Karan Ahuja, Ruofei Du, Andrea Colaço, Johnny Lee, Mar Gonzalez-Franco, and David Kim. 2024. Augmented Object Intelligence with XR-Objects. (Aug. 2024). [https://doi.org/10.1145/3654777.](https://doi.org/10.1145/3654777.3676379) [3676379](https://doi.org/10.1145/3654777.3676379) arXiv:2404.13274 [cs].
- <span id="page-20-19"></span>[34] Fiona Draxler, Julia Maria Brenner, Manuela Eska, Albrecht Schmidt, and Lewis L Chuang. 2022. Agenda- and Activity-Based Triggers for Microlearning. In 27th International Conference on Intelligent User Interfaces. ACM, Helsinki Finland, 620–632. <https://doi.org/10.1145/3490099.3511133>
- <span id="page-20-15"></span>[35] Fiona Draxler, Audrey Labrie, Albrecht Schmidt, and Lewis L. Chuang. 2020. Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20). Association for Computing Machinery, New York, NY, USA, 1–12. <https://doi.org/10.1145/3313831.3376537>
- <span id="page-20-11"></span>[36] Darren Edge, Elly Searle, Kevin Chiu, Jing Zhao, and James A. Landay. 2011. MicroMandarin: mobile language learning in context. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, Vancouver BC Canada, 3169–3178. <https://doi.org/10.1145/1978942.1979413>
- <span id="page-20-25"></span>[37] Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, Cheng Peng, Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso, Derek Valleroy, Dinesh Ginjupalli, Duncan Frost, Edward Miller, Elias Mueggler, Evgeniy Oleinik, Fan Zhang, Guruprasad Somasundaram, Gustavo Solaira, Harry Lanaras, Henry Howard-Jenkins, Huixuan Tang, Hyo Jin Kim, Jaime Rivera, Ji Luo, Jing Dong, Julian Straub, Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira, Mark Schwesinger, Maurizio Monge, Nan Yang, Nick Charron, Nikhil Raina, Omkar Parkhi, Peter Borschowa, Pierre Moulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington, Sachin Kulkarni, Sagar Miglani, Santosh Gondi, Saransh Solanki, Sean Diener, Shangyi Cheng, Simon Green, Steve Saarinen, Suvam Patra, Tassos Mourikis, Thomas Whelan, Tripti Singh, Vasileios Balntas, Vijay Baiyya, Wilson Dreewes, Xiaqing Pan, Yang Lou, Yipu Zhao, Yusuf Mansour, Yuyang Zou, Zhaoyang Lv, Zijian Wang, Mingfei Yan, Carl Ren, Renzo De Nardi, and Richard Newcombe. 2023. Project Aria: A New Tool for Egocentric Multi-Modal AI Research. arXiv[:2308.13561](https://arxiv.org/abs/2308.13561) [cs.HC]
- <span id="page-20-2"></span>[38] Michael Eraut \*. 2004. Informal learning in the workplace. Studies in Continuing Education 26, 2 (July 2004), 247–273. [https://doi.org/10.1080/](https://doi.org/10.1080/158037042000225245) [158037042000225245](https://doi.org/10.1080/158037042000225245)
- <span id="page-20-4"></span>[39] Google. 2024. Project Astra. [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/project-astra/) [project-astra/](https://deepmind.google/technologies/gemini/project-astra/)
- <span id="page-20-26"></span>[40] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. 2022. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18995–19012.
- <span id="page-20-17"></span>[41] Juhye Ha, Hyeon Jeon, Daeun Han, Jinwook Seo, and Changhoon Oh. 2024. CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24). Association for Computing Machinery, New York, NY, USA, 1–24. <https://doi.org/10.1145/3613904.3642472>
- <span id="page-20-36"></span>[42] Sandra G. Hart. 2006. Nasa-Task Load Index (NASA-TLX); 20 Years Later. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 50, 9 (2006), 904–908. <https://doi.org/10.1177/154193120605000909>
- <span id="page-20-12"></span>[43] Ari Hautasaari, Takeo Hamada, Kuntaro Ishiyama, and Shogo Fukushima. 2020. VocaBura: A Method for Supporting Second Language Vocabulary Learning While Walking. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 4 (Sept. 2020), 135:1–135:23. <https://doi.org/10.1145/3369824>
- <span id="page-20-29"></span>[44] Caroline Haythornthwaite. 2022. Analytics for informal learning in social media. Handbook of Learning Analytics (2022), 163.
- <span id="page-20-31"></span>[45] Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Pittsburgh, Pennsylvania, USA) (CHI '99). Association for Computing Machinery, New York,

NY, USA, 159–166. <https://doi.org/10.1145/302979.303030>

- <span id="page-20-32"></span>[46] Human Science. [n. d.]. Personal Values. [https://humanscience.fandom.com/](https://humanscience.fandom.com/wiki/Personal_values) [wiki/Personal\\_values.](https://humanscience.fandom.com/wiki/Personal_values) [Accessed: 2024-12-02].
- <span id="page-20-28"></span>[47] Nuwan Janaka, Runze Cai, Ashwin Ram, Lin Zhu, Shengdong Zhao, and Yong Kai Qi. 2024. PilotAR: Streamlining Pilot Studies with OHMDs from Concept to Insight. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (Sept. 2024). <https://doi.org/10.1145/3678576>
- <span id="page-20-35"></span>[48] Nuwan Janaka, Jie Gao, Lin Zhu, Shengdong Zhao, Lan Lyu, Peisen Xu, Maximilian Nabokow, Silang Wang, and Yanch Ong. 2023. GlassMessaging: Towards Ubiquitous Messaging Using OHMDs. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 7, 3 (Sept. 2023). <https://doi.org/10.1145/3610931>
- <span id="page-20-34"></span>[49] Nuwan Janaka, Chloe Haigh, Hyeongcheol Kim, Shan Zhang, and Shengdong Zhao. 2022. Paracentral and near-peripheral visualizations: Towards attentionmaintaining secondary information presentation on OHMDs during in-person social interactions. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI '22). Association for Computing Machinery, New York, NY, USA, 1–14. <https://doi.org/10.1145/3491102.3502127>
- <span id="page-20-13"></span>[50] Nuwan Janaka, Xinke Wu, Shan Zhang, Shengdong Zhao, and Petr Slovak. 2022. Visual Behaviors and Mobile Information Acquisition. [https://doi.org/10.48550/](https://doi.org/10.48550/arXiv.2202.02748) [arXiv.2202.02748](https://doi.org/10.48550/arXiv.2202.02748)
- <span id="page-20-23"></span>[51] Nuwan Janaka, Shengdong Zhao, David Hsu, Sherisse Tan Jing Wen, and Chun Keat Koh. 2024. TOM: A Development Platform For Wearable Intelligent Assistants. In Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing Pervasive and Ubiquitous Computing. ACM. <https://doi.org/10.1145/3675094.3678382>
- <span id="page-20-33"></span>[52] Nuwan Janaka, Shengdong Zhao, and Shardul Sapkota. 2023. Can Icons Outperform Text? Understanding the Role of Pictograms in OHMD Notifications. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 575, 23 pages. <https://doi.org/10.1145/3544548.3580891>
- <span id="page-20-9"></span>[53] Peter Jarvis. 2009. Learning from everyday life. In The Routledge international handbook of lifelong learning. Routledge, 19–30.
- <span id="page-20-20"></span>[54] Smita Jhajharia, D. S. K. Pal, and Dr Seema Verma. 2014. Wearable Computing and its Application. In International Journal of Computer Science and Information Technologie. <https://api.semanticscholar.org/CorpusID:541229>
- <span id="page-20-0"></span>[55] R Kaplan. 1989. The experience of nature: A psychological perspective. Cambridge University Perss (1989).
- <span id="page-20-6"></span>[56] Christopher A. Kelly and Tali Sharot. 2021. Individual differences in informationseeking. Nature Communications 12, 1 (Dec. 2021). [https://doi.org/10.1038/](https://doi.org/10.1038/s41467-021-27046-5) [s41467-021-27046-5](https://doi.org/10.1038/s41467-021-27046-5) Publisher: Nature Publishing Group.
- <span id="page-20-14"></span>[57] Minyeong Kim, Jiwook Lee, Youngji Koh, Chanhee Lee, Uichin Lee, and Auk Kim. 2024. Interrupting for Microlearning: Understanding Perceptions and Interruptibility of Proactive Conversational Microlearning Services. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24). Association for Computing Machinery, New York, NY, USA, 1–21. <https://doi.org/10.1145/3613904.3642778>
- <span id="page-20-7"></span>[58] Denis Kotkov, Alan Medlar, and Dorota Glowacka. 2023. Rethinking Serendipity in Recommender Systems. In Proceedings of the 2023 Conference on Human Information Interaction and Retrieval (Austin, TX, USA) (CHIIR '23). Association for Computing Machinery, New York, NY, USA, 383–387. [https://doi.org/10.](https://doi.org/10.1145/3576840.3578310) [1145/3576840.3578310](https://doi.org/10.1145/3576840.3578310)
- <span id="page-20-30"></span>[59] David R. Krathwohl. 2002. A Revision of Bloom's Taxonomy: An Overview. Theory Into Practice 41, 4 (2002), 212–218. [https://doi.org/10.1207/s15430421tip4104\\_](https://doi.org/10.1207/s15430421tip4104_2) [2](https://doi.org/10.1207/s15430421tip4104_2)
- <span id="page-20-24"></span>[60] Nallapaneni Manoj Kumar, P. Ranjith Krishna, Pavan Kumar Pagadala, and N. M. Saravana Kumar. 2018. Use of Smart Glasses in Education-A Study. In 2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on. IEEE, Palladam, India. [https://doi.org/10.1109/I-](https://doi.org/10.1109/I-SMAC.2018.8653666)[SMAC.2018.8653666](https://doi.org/10.1109/I-SMAC.2018.8653666)
- <span id="page-20-5"></span>[61] Jaewook Lee, Jun Wang, Elizabeth Brown, Liam Chu, Sebastian S. Rodriguez, and Jon E. Froehlich. 2024. GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 408, 20 pages. <https://doi.org/10.1145/3613904.3642230>
- <span id="page-20-10"></span>[62] Yen-Mei Lee. 2023. Mobile microlearning: a systematic literature review and its implications. Interactive Learning Environments 31, 77 (Oct. 2023), 4636–4651. <https://doi.org/10.1080/10494820.2021.1977964>
- <span id="page-20-27"></span>[63] Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, and Michelle Li. 2024. OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 8, 22 pages. [https://doi.](https://doi.org/10.1145/3613904.3642068) [org/10.1145/3613904.3642068](https://doi.org/10.1145/3613904.3642068)
- <span id="page-20-3"></span>[64] Emily G. Liquin and Alison Gopnik. 2022. Children are more exploratory and learn more than adults in an approach-avoid task. Cognition 218 (2022), 104940.

<https://doi.org/10.1016/j.cognition.2021.104940>

- <span id="page-21-41"></span>[65] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems 36 (2024).
- <span id="page-21-26"></span>[66] David W Livingstone. 2001. Adults' informal learning: Definitions, findings, gaps and future research. (2001).
- <span id="page-21-8"></span>[67] D. W. Livingstone. 2001. Adults' Informal Learning: Definitions, Findings, Gaps, and Future Research. NALL Working Paper #21. Technical Report. NALL: New Approaches to Lifelong Learning, Ontario Institute for Studies in Education, University of Toronto, 252 Bloor Street West, Toronto, Ontario, Canada M5S 1V6. <https://eric.ed.gov/?id=ED452390>
- <span id="page-21-21"></span>[68] Wulf Loh and Catrin Misselhorn. 2019. Augmented learning, smart glasses and knowing how. AI & SOCIETY (March 2019). [https://doi.org/10.1007/s00146-](https://doi.org/10.1007/s00146-019-00881-3) [019-00881-3](https://doi.org/10.1007/s00146-019-00881-3)
- <span id="page-21-3"></span>[69] Arien Mack. 2003. Inattentional blindness: Looking without seeing. Current directions in psychological science 12, 5 (2003), 180–184.
- <span id="page-21-30"></span>[70] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753 (2024).
- <span id="page-21-27"></span>[71] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. 3195–3204.
- <span id="page-21-0"></span>[72] Victoria J. Marsick and Karen E. Watkins. 2001. Informal and Incidental Learning. New Directions for Adult and Continuing Education 2001, 89 (2001), 25–34. [https:](https://doi.org/10.1002/ace.5) [//doi.org/10.1002/ace.5](https://doi.org/10.1002/ace.5)
- <span id="page-21-11"></span>[73] Richard Mayer (Ed.). 2014. The Cambridge Handbook of Multimedia Learning (2 ed.). Cambridge University Press, Cambridge. [https://doi.org/10.1017/](https://doi.org/10.1017/CBO9781139547369) [CBO9781139547369](https://doi.org/10.1017/CBO9781139547369)
- <span id="page-21-35"></span>[74] Alireza Modirshanechi, Johanni Brea, and Wulfram Gerstner. 2022. A taxonomy of surprise definitions. Journal of Mathematical Psychology 110 (2022), 102712.
- <span id="page-21-4"></span>[75] R Bruce Money and John C Crotts. 2003. The effect of uncertainty avoidance on information search, planning, and purchases of international travel vacations. Tourism Management 24, 2 (2003), 191–202.
- <span id="page-21-14"></span>[76] Nicolae Moroianu, Silvia-Elena Iacob, and Alexandra Constantin. 2023. Artificial Intelligence in Education: a Systematic Review. 906–921. [https://doi.org/10.](https://doi.org/10.2478/9788367405546-084) [2478/9788367405546-084](https://doi.org/10.2478/9788367405546-084)
- <span id="page-21-33"></span>[77] Donald A. Norman. 2013. The design of everyday things (revised and expanded edition ed.). Basic Books.
- <span id="page-21-24"></span>[78] Anne Oeldorf-Hirsch. 2018. The role of engagement in learning from active and incidental news exposure on social media. Mass communication and society 21, 2 (2018), 225–247.
- <span id="page-21-39"></span>[79] Veljko Pejovic and Mirco Musolesi. 2014. InterruptMe: designing intelligent prompting mechanisms for pervasive applications. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp '14). Association for Computing Machinery, New York, NY, USA, 897–908. <https://doi.org/10.1145/2632048.2632062>
- <span id="page-21-17"></span>[80] Pavan Kartheek Rachabatuni, Filippo Principi, Paolo Mazzanti, and Marco Bertini. 2024. Context-aware chatbot using MLLMs for Cultural Heritage. In Proceedings of the ACM Multimedia Systems Conference 2024 on ZZZ. ACM, Bari Italy, 459–463. <https://doi.org/10.1145/3625468.3652193>
- <span id="page-21-15"></span>[81] Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.
- <span id="page-21-34"></span>[82] Laria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI EA '21). Association for Computing Machinery, New York, NY, USA, Article 314, 7 pages. <https://doi.org/10.1145/3411763.3451760>
- <span id="page-21-18"></span>[83] Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, and Abhinav Sethy. 2023. Integrating summarization and retrieval for enhanced personalization via large language models. In CIKM 2023 Workshop Personalized Generative AI. [https://www.amazon.science/publications/integrating-summarization-and](https://www.amazon.science/publications/integrating-summarization-and-retrieval-for-enhanced-personalization-via-large-language-models)[retrieval-for-enhanced-personalization-via-large-language-models](https://www.amazon.science/publications/integrating-summarization-and-retrieval-for-enhanced-personalization-via-large-language-models)
- <span id="page-21-5"></span>[84] Leonid Rozenblit and Frank Keil. 2002. The misunderstood limits of folk science: An illusion of explanatory depth. Cognitive science 26, 5 (2002), 521–562.
- <span id="page-21-23"></span>[85] R. Saravanan. 2021. The Rumsfeld Matrix: Degrees of Knowledge. In The Climate Demon: Past, Present, and Future of Climate Prediction, R. Saravanan (Ed.). Cambridge University Press, Cambridge, 193–211. [https://doi.org/10.](https://doi.org/10.1017/9781009039604.017) [1017/9781009039604.017](https://doi.org/10.1017/9781009039604.017)
- <span id="page-21-12"></span>[86] R. Keith Sawyer (Ed.). 2022. The Cambridge Handbook of the Learning Sciences (3 ed.). Cambridge University Press, Cambridge. [https://doi.org/10.1017/](https://doi.org/10.1017/9781108888295) [9781108888295](https://doi.org/10.1017/9781108888295)
- <span id="page-21-28"></span>[87] Shalom H. Schwartz. 1992. Universals in the Content and Structure of Values: Theoretical Advances and Empirical Tests in 20 Countries. Advances in Experimental Social Psychology, Vol. 25. Academic Press, 1–65. [https:](https://doi.org/10.1016/S0065-2601(08)60281-6) [//doi.org/10.1016/S0065-2601\(08\)60281-6](https://doi.org/10.1016/S0065-2601(08)60281-6)
- <span id="page-21-6"></span>[88] Tali Sharot and Cass R Sunstein. 2020. How people decide what they want to know. Nature Human Behaviour 4, 1 (2020), 14–19.
- <span id="page-21-25"></span>[89] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. 2023. What does CLIP know about a red circle? Visual prompt engineering for VLMs. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 11987–11997.
- <span id="page-21-2"></span>[90] Peter Sommerauer and Oliver Müller. 2014. Augmented Reality in Informal Learning Environments: A Field Experiment in a Mathematics Exhibition. Computers & Education in press (Oct. 2014). [https://doi.org/10.1016/j.compedu.2014.](https://doi.org/10.1016/j.compedu.2014.07.013) [07.013](https://doi.org/10.1016/j.compedu.2014.07.013)
- <span id="page-21-7"></span>[91] Donggil Song and Curtis J. Bonk. 2016. Motivational factors in self-directed informal learning from online learning resources. Cogent Education 3, 1 (Dec. 2016). <https://doi.org/10.1080/2331186X.2016.1205838>
- <span id="page-21-19"></span>[92] Guangzhi Sun, Xiao Zhan, and Jose Such. 2024. Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents. In Proceedings of the 6th ACM Conference on Conversational User Interfaces (CUI '24). Association for Computing Machinery, New York, NY, USA, 1–6. <https://doi.org/10.1145/3640794.3665887>
- <span id="page-21-40"></span>[93] Aik Lim Tan, Robyn Gillies, and Azilawati Jamaludin. 2021. A Case Study: Using a Neuro-Physiological Measure to Monitor Students' Interest and Learning during a Micro:Bit Activity. Education Sciences 11, 8 (2021). [https://doi.org/10.](https://doi.org/10.3390/educsci11080379) [3390/educsci11080379](https://doi.org/10.3390/educsci11080379)
- <span id="page-21-37"></span>[94] Felicia Fang-Yi Tan, Peisen Xu, Ashwin Ram, Wei Zhen Suen, Shengdong Zhao, Yun Huang, and Christophe Hurter. 2024. AudioXtend: Assisted Reality Visual Accompaniments for Audiobook Storytelling During Everyday Routine Tasks. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 83, 22 pages. <https://doi.org/10.1145/3613904.3642514>
- <span id="page-21-31"></span>[95] Kristin Vadas, Kent Lyons, Daniel Ashbrook, Ji Soo Yi, Thad Starner, and Julie Jacko. 2006. Reading on the Go: An Evaluation of Three Mobile Display Technologies. (2006), 16.
- <span id="page-21-32"></span>[96] Kristin Vadas, Nirmal Patel, Kent Lyons, Thad Starner, and Julie Jacko. 2006. Reading on-the-go: a comparison of audio and hand-held displays. In Proceedings of the 8th conference on Human-computer interaction with mobile devices and services (MobileHCI '06). Association for Computing Machinery, New York, NY, USA, 219–226. <https://doi.org/10.1145/1152215.1152262>
- <span id="page-21-38"></span>[97] Jaan Valsiner. 1997. Culture and the development of children's action: A theory of human development, 2nd ed. John Wiley & Sons Inc, Hoboken, NJ, US. Pages: xx, 364.
- <span id="page-21-10"></span>[98] Christian David Vazquez, Afika AyandaNyati, Alexander Luh, Megan Fu, Takako Aikawa, and Pattie Maes. 2017. Serendipitous Language Learning in Mixed Reality. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17). Association for Computing Machinery, New York, NY, USA, 2172–2179. <https://doi.org/10.1145/3027063.3053098>
- <span id="page-21-36"></span>[99] Fengjie Wang, Yanna Lin, Leni Yang, Haotian Li, Mingyang Gu, Min Zhu, and Huamin Qu. 2024. OutlineSpark: Igniting AI-powered Presentation Slides Creation from Computational Notebooks through Outlines. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 159, 16 pages. <https://doi.org/10.1145/3613904.3642865>
- <span id="page-21-1"></span>[100] Zeyu Wang, Yuanchun Shi, Yuntao Wang, Yuchen Yao, Kun Yan, Yuhan Wang, Lei Ji, Xuhai Xu, and Chun Yu. 2024. G-VOILA: Gaze-Facilitated Information Querying in Daily Scenarios. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 2, Article 78 (may 2024), 33 pages. <https://doi.org/10.1145/3659623>
- <span id="page-21-16"></span>[101] Zhan Wang, Lin-Ping Yuan, Liangwei Wang, Bingchuan Jiang, and Wei Zeng. 2024. VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models. (Jan. 2024). [https://doi.org/10.1145/](https://doi.org/10.1145/3613904.3642235) [3613904.3642235](https://doi.org/10.1145/3613904.3642235) arXiv:2401.11923 [cs].
- <span id="page-21-22"></span>[102] Maheshya Weerasinghe, Verena Biener, Jens Grubert, Aaron Quigley, Alice Toniolo, Klen Čopič Pucihar, and Matjaž Kljun. 2022. VocabulARy: Learning Vocabulary in AR Supported by Keyword Visualisations. IEEE Transactions on Visualization and Computer Graphics 28, 11 (Nov. 2022), 3748–3758. [https:](https://doi.org/10.1109/TVCG.2022.3203116) [//doi.org/10.1109/TVCG.2022.3203116](https://doi.org/10.1109/TVCG.2022.3203116) Publisher: IEEE Computer Society.
- <span id="page-21-29"></span>[103] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824–24837.
- <span id="page-21-20"></span>[104] Zhenyu Xu, Hailin Xu, Zhouyang Lu, Yingying Zhao, Rui Zhu, Yujiang Wang, Mingzhi Dong, Yuhu Chang, Qin Lv, Robert P. Dick, Fan Yang, Tun Lu, Ning Gu, and Li Shang. 2024. Can Large Language Models Be Good Companions? An LLM-Based Eyewear System with Conversational Common Ground. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 2, Article 87 (may 2024), 41 pages. <https://doi.org/10.1145/3659600>
- <span id="page-21-9"></span>[105] Saadiah Yahya, Erny Ahmad, and Kamarularifin Abd Jalil. 2010. The definition and characteristics of ubiquitous learning: A discussion. International Journal of Education and Development using ICT 6, 11 (March 2010). [https://www.](https://www.learntechlib.org/p/188069/) [learntechlib.org/p/188069/](https://www.learntechlib.org/p/188069/)
- <span id="page-21-13"></span>[106] Keiji Yanai, Takuma Maruyama, and Yoshiyuki Kawano. 2014. A Cooking Recipe Recommendation System with Visual Recognition of Food Ingredients.

International Journal of Interactive Mobile Technologies 8, 2 (2014).

- <span id="page-22-1"></span>[107] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). arXiv[:2309.17421](https://arxiv.org/abs/2309.17421) [cs.CV]
- <span id="page-22-0"></span>[108] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A Survey on Multimodal Large Language Models. arXiv[:2306.13549](https://arxiv.org/abs/2306.13549) [cs.CV]
- <span id="page-22-11"></span>[109] Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, and Michael Bendersky. 2022. Retrieval-Enhanced Machine Learning. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR '22). Association for Computing Machinery, New York, NY, USA, 2875–2886. [https://doi.org/10.1145/3477495.](https://doi.org/10.1145/3477495.3531722) [3531722](https://doi.org/10.1145/3477495.3531722)
- <span id="page-22-7"></span>[110] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 (2023).
- <span id="page-22-2"></span>[111] Shengdong Zhao, Felicia Tan, and Katherine Fennedy. 2023. Heads-Up Computing Moving Beyond the Device-Centered Paradigm. Commun. ACM 66, 9 (aug 2023), 56–63. <https://doi.org/10.1145/3571722>
- <span id="page-22-14"></span>[112] Wazeer Deen Zulfikar, Samantha Chan, and Pattie Maes. 2024. Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 1–18. <https://doi.org/10.1145/3613904.3642450>

#### A Formative Study

#### <span id="page-22-3"></span>A.1 Data Analysis

We conducted a thematic analysis following the approach by Braun and Clarke [\[18\]](#page-19-19) on 12 sets of transcribed user responses, supplemented by observational notes on user behaviors and environments. Initially, two co-authors independently reviewed 4 sets of notes. They generated initial codes, grouping them into themes aligned with the research questions. With an initial agreement of 85%, they resolved discrepancies through discussion. One co-author then coded the remaining data, refining themes until saturation. Both co-authors reviewed the textual data and video footage to extract relevant quotes for each theme.

#### <span id="page-22-4"></span>A.2 Knowledge Categories and Examples

Table [4](#page-22-15) shows the definitions and examples of four types of knowledge based on Revised Bloom's Taxonomy [\[59\]](#page-20-30).

<span id="page-22-15"></span>Table 4: Knowledge Types with Definitions and Examples for Beverage Scene in a Supermarket

| Knowledge Type       | Definition                                                                                                              | Examples                                                                                                                                                                                                 |
|----------------------|-------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Factual Knowledge    | Knowledge of terminology and<br>specific details and elements                                                           | "This apple juice contains natural antioxidants called<br>polyphenols, which can help protect your cells from<br>damage and may even support heart health"                                               |
| Conceptual Knowledge | Knowledge of classifications,<br>principles, and interrelation<br>ships among elements                                  | "Apple juice generally has a sweeter taste and more<br>calories compared to orange juice, which is higher in<br>vitamin C and slightly more acidic, making it better for<br>boosting the immune system." |
| Procedural Knowledge | Knowledge of processes, meth<br>ods, and criteria for using ap<br>propriate procedures                                  | "Create a unique mocktail by mixing apple juice with a<br>splash of orange juice and ginger ale, then garnish with<br>a mint leaf for a refreshing twist."                                               |
|                      | Metacognitive Knowledge Knowledge of cognition, in<br>cluding strategic thinking, task<br>awareness, and self-knowledge | "Reflecting on why I choose certain brands or what crite<br>ria I prioritize can help me understand my own decision<br>making processes."                                                                |

#### B AiGet System

#### <span id="page-22-5"></span>B.1 FPV Difference Calculation

The system calculatesthe FPV difference using an Image Embedding Model (MobileNet) from MediaPipe to determine the feature similarity between FPV frames within a sliding window of 16 frames (of 5 seconds). LLM requests are triggered if 80% of the corresponding

frames have a cosine similarity below 0.6, ensuring that prompts are generated only when the user's view has significantly changed. The thresholds for time and similarity were optimized through pilot testing to ensure a balance between information richness and system processing time.

#### <span id="page-22-6"></span>B.2 User Profile Sample

A sample user profile is as follows: {"Values/Interest": ["healthy life", "fitness", "flowers", "coffee lover", "cat", "fun facts/history", "design"], "Age": "30", "Gender": "female", "Citizenship": "XX", "Residence": "XX", "Education": "PhD in visual design", "Occupation": "senior student in XX University"}.

### <span id="page-22-9"></span>B.3 Parallel Processing during Output Transformation

To optimize processing time, two agents work in parallel during output transformation. The Image Reference Agent selects the clearest image covering the mentioned entities from 16 FPV frames and returns their bounding boxes. Concurrently, the Text+Audio Transformation Agent transforms knowledge candidates into two lists: one extracting keywords from sentences and pairing them with relevant emojis for quick comprehension, and another providing detailed, engaging voiceovers. Additionally, to minimize language barriers, the Text+Audio Transformation Agent transcribes the knowledge into the user's preferred languag[e12.](#page-22-16)

#### <span id="page-22-10"></span>B.4 Implementation Details

Table [5](#page-23-2) demonstrates the details of AiGet system implementations using various hardware and software.

#### <span id="page-22-8"></span>C Prompts for Large Language Models

### C.1 Prompt for AiGet System's Context Analysis

Figure [10](#page-23-3) shows the prompt of AiGet system for moment analysis.

### C.2 Prompt for AiGet System's Knowledge Generation and Prioritization

Figure [11](#page-24-0) shows the prompts used to generate knowledge and score them in the AiGet system to satisfy the preferences of different users.

### C.3 Prompt for AiGet System's Output Transformation

Figure [12](#page-25-0) shows the prompt for transforming the knowledge into different output formats for two agents.

#### <span id="page-22-12"></span>C.4 Prompt for Baseline w/o R

Figure [13](#page-25-1) shows the prompt for Baseline w/o R.

#### <span id="page-22-13"></span>C.5 Prompt for Baseline w/o RP

Figure [14](#page-25-2) shows the prompt for the prompt for Baseline w/o RP.

<span id="page-22-16"></span><sup>12</sup>Translation occurs in the final LLM pipeline step, with earlier stages in English, due to observed limitations in generated knowledge quality when using other languages during pilot testing.

| Component           | Description                                                                                   | Associated Technologies/Tools                      |  |
|---------------------|-----------------------------------------------------------------------------------------------|----------------------------------------------------|--|
| AiGet               | Main system developed for the application.                                                    | Python 3.9                                         |  |
| OHMD (Xreal Air) UI | Interfaces with a laptop for near-eye display.                                                | Tkinter                                            |  |
| Pupil Core          | Facilitates gaze detection and FPV video stream                                               | Socket connection with Pupil Capture App           |  |
|                     | ing.                                                                                          |                                                    |  |
|                     |                                                                                               | FPV Embedding (Difference): MediaPipe (MobileNet), |  |
| Software Backend    |                                                                                               | Voice Transcription: Distil-whisper,               |  |
|                     | Handles multiple context data concurrently.<br>Calculates knowledge similarity for filtering. | Location: Geopy 2.3.0, Geocoder 1.38.1,            |  |
|                     |                                                                                               | Time: Python's datetime,                           |  |
|                     |                                                                                               | Text Embedding (Difference): all-MiniLM-L6-v2      |  |
|                     |                                                                                               | Context Analysis: Gemini 1.5 Flash                 |  |
| LLMs                | Processes multimodal data to describe context,                                                | Knowledge Generation: Gemini 1.5 Pro               |  |
|                     | generate knowledge, and formulate output.                                                     | Output Transformation: Gemini 1.5 Flash            |  |
| Prompt Engineering  | Ensures efficient task performance and seamless                                               | Multi-Agent Approach,                              |  |
|                     |                                                                                               | Few-shot prompts,                                  |  |
|                     |                                                                                               | JSON formatted responses,                          |  |
|                     | integration.                                                                                  | Chain-of-Thought                                   |  |

#### Table 5: Key Components of the AiGet System and Associated Technologies.

<span id="page-23-3"></span>Role: You are an intelligent assistant equipped to enhance informal learning through real-time, context-sensitive interactions. Utilize input data, including first-person view (FPV) with gaze position markers (red dot/circle on image), ambient audio, current time, and location to gauge the user's immediate environment. Generate responses considering user profile of their interests and expertise. ======================

Task: 1. Environment Depiction: Detail the user's current surroundings, activities, primary focus, and peripheral elements using input from images or contextual data. Extract & Analyze text & image if the user is reading something. - Based on user profile, when generating activity description, consider whether user is staying in a familiar or unfamiliar environment (e.g., based on if their current location is in user's home, school, or residence country/city).

2. Gaze Analysis: Identify the user's current mode based on their gaze pattern in FPV: Saccade: Random Gaze movements, typical during casual walking or commuting, without a specific focused object.

Quick Browse: Quick scanning of multiple semantic-related objects while walking; not a random saccade but more like checking objects along a certain path. This often indicates that the user is scanning the object they already know. Focused: Consistent focus on a specific object (or focus on \*specific kind of objects\*, e.g., different milks, with gaze switch back and forth) across multiple frames. This indicates that the user is thinking and potentially decision-making.

3. Entity Identification: \*\*Try your best to Identify the specific brand, model, or product name of object (e.g., Sony A7C2 instead of camera); breed and exact species of animals (e.g., Dachshund instead of "Dog") and planets (e.g., Tembusu instead of "tree") in the environment; name of audio/music and buildings. Rather than a generic term, e.g., green plant\*\* Primary Entity: Determine the main focus (gazed for long time or currently interact with like touching) of the user's attention (physical or virtual entity, e.g., article, music, or the unfamiliar place/location). Using OCR if possible to detail the text if the user is reading. Describe its color, size, and other features if possible.\*Return specific species of the plants or animals if involved.\* \*Note: if user is asking question, identify the specific object or entity they are asking about, and use it as primary entity.\*

Peripheral Entities: Identify \*no more than 4\* related or most interesting entities near the primary focus (physical or virtual, e.g., article, music), or the current place/location if they are interesting or unfamiliar, or any objects aligned with user interest. Using OCR if possible to detail if any. Describe its color, size, and other features if possible. \*Return specific species of the plants/animals/products if involved, and try to avoid vague terms, like "various plants".\* For Peripheral Entities, you can consider user profile to find the unnoticed entities that align with user interest (e.g., if users like plants, then try you best to identify if any plants and what are their species in the environment). \* Perform OCR (in any language, e.g., English, Chinese, etc) to read the text on the target object and identify the correct target. \* Predict familiar entities based on user profile, e.g., a Chinese Student may not be familiar with Indian brands.

Describe the entity's location reference to users, "e.g., under the table, on the middle layer of shelf, in the fridge, etc."

\*Most Important: The entity should focus on the MAIN or MOST interesting stuff that aligns with user intention and in-situ context. For example, don't mention the glasses if user is seeing the specimen behind the glasses in museum, or don't mention shelf as entity if user is focusing on the product on the shelf during shopping, or don't mention wall & celling design if user is looking at the poster on the wall, or the entity is not signpost but the content on it.\*

4. Learning Goals Prediction: Predict the user's learning objectives, whether they are trying to understand unfamiliar stuff in front, gathering fun facts, making decisions related to their current activities, acquiring skills for future use, satisfying their life value, or asking specific questions (towards specific verbal mentioned or gazed entity). 5. Consider History if User asks a question (Optional): If the user asks a question, analyze if user is asking a follow up question about provided knowledge in previous moment (by using provided context) or simply asking new questions.

#### [USER\_PROFILE\_PLACEHOLDER] ===================== Output format: ```json "Context Description": { "Loc & Time": "", "Gaze Pattern": "[Gaze Pattern] | [Reason]", "Activity": "[Familiar/Unfamiliar] Env | [Activity Description]", "OCR": "XXX", "Primary Focus": "Specific Object: Short Description (color, size, and other special features), Location; familiarity", "Peripheral Entities": ["A Specific Object \*(don't use vague terms, e.g., "various plants", but be specific)\*: Description, Location; familiarity", "A Specific Object: Description, Location; familiarity"], "Predicted Intention": "", "Any Potential angle to Align with personal interest": "[Yes/No] | [Value/Interest from User Profile]" Example Response: ```json "Context Description": { "Loc & Time": "Cold Storage, 15:30", "Gaze Pattern": "Focused | The user is focusing on various fruits in multiple frames.", "Activity": "Unfamiliar Env | The user is actively scanning the fruit section, focusing momentarily on various fruits", "OCR": "Driscoll's\nBlueberry", "Primary Focus": "Driscoll's Blueberry: on the top shelf you just passed by; familiar", "Peripheral Entities": ["Organic Strawberry: on the middle shelf you just passed by; familiar", "Conventional Kiwi: on the right side of strawberries; familiar", "Cold Storage: current stayed location; unfamiliar"], "Predicted Intention": "Make quick decisions on what fruit to buy.",

#### Figure 10: Context Analysis Prompt of AiGet system.

#### D In-Lab and Real-World Study

#### <span id="page-23-0"></span>D.1 Evaluation Measures

"Any Potential angle to Align with personal interest": "Yes | health, eating"

Table [6](#page-26-1) presents the subjective measures used in in-lab and realworld evaluations.

#### D.3 Figures for Statistics Results

Figure [15](#page-26-3) presents both a bar chart and a line chart illustrating Naturalness ratings across different scenarios and days.

### <span id="page-23-1"></span>D.2 Demographics for Participants in Real-World Study's Phase 2 stage.

Table [7](#page-26-2) shows the demographics for participants in real-world study's Phase 2 stage.

<span id="page-23-2"></span>

#### CHI '25, April 26–May 01, 2025, Yokohama, Japan Cai et al.

<span id="page-24-0"></span>Role: You are an intelligent assistant equipped to enhance informal learning through real-time, context-sensitive interactions. Utilize input data, including first-person view (FPV) with attention info, ambient audio, current time, and location to gauge the user's immediate environment. Generate responses to deepen the user's knowledge, tailored to user profile. Task: **1. Analyze user profile and their current in-situ context:** Predict the user's potential learning desires, e.g., gathering interesting facts, making decisions related to current activities, acquiring future skills, or satisfying life values. Align responses with the following user's profile and values, prioritizing valued content but not limiting to it. [USER\_PROFILE\_PLACEHOLDER] **2. Knowledge Analysis:** Provide interesting knowledge on primary and semantic-related peripheral entities related to the user's current focus. Ensure it enhances interest, expands knowledge, and includes serendipitous information. Avoid repetition of recent topics. Consider following types: -Factual Knowledge: \*Less known but interesting\* facts or common misconceptions for common and familiar objects to users (don't provide generic and boring knowledge that users probably know). \*Provide useful and interesting introduction to the entity if it's unfamiliar or uncommon for users.\* Link the unfamiliar knowledge to user familiar knowledge if user is staying in unfamiliar places. -Conceptual Knowledge: Interesting concepts related to the entity, try to \*link to similar nearby entities.\* For example, when user is making decisions, provide comparative information between entities to support decision-making and align with user values (e.g., health life). -Procedural Knowledge: Useful step-by-step instructions or methods related to the entity. \*\*All knowledge should be novel and can open user mind. Avoid providing basic knowledge that neither help user open perspective nor useful.\*\* \*\*When generating the knowledge, try to maximize the scores in terms of (Novelty, Alignment with Values, Utility, Unexpectedness) in instruction no.4.\*\* \*\*Check the conversation history. Avoid providing similar topics shown in history (try to \*present different knowledge about the entity that appears in history\* but also not so frequent). Or repeatedly say knowledge about same entities.\*\* **3.Fine-tune the knowledge based on user's profile and values:** - Use personalized info to predict their familiarity with the entity and interest level. - If user is familiar with the entity, provide more advanced or specific unknown unknown knowledge (for example, "palm trees play a crucial role in the ecosystem by providing habitat and food for various species" is too simple for PhD. tell "Palm trees emit volatile organic compounds that attract beneficial insects to control pests, reducing the need for pesticides in agriculture." instead or even share what the compounds are for chemical students) that surprises users or useful tips (procedural knowledge) that are helpful to ongoing or future tasks. - If user is unfamiliar with the entity, you can try to link it with familiar knowledge, e.g., if user sees a foreign instant noodle, you can tell noodle's unique flavor, and how it compared to the instant noodle that user may be familiar considering where the user from. - You can try to link the current entities with user potential interest (Can consider "Any Potential angle to Align with personal interest" from input). For example, if user is interested in health, you can provide knowledge about the health benefits of the fruits user is looking at. Also adapt the knowledge consider user gender, age, education level. **4. Generate and Evaluate Knowledge:** \*Generate Knowledge tailored to above user profile (e.g., education level, culture background, etc.)\*, context, intention. Then, evaluate each piece using the following criteria considering the user's profile and values: - Novelty (0 or 1): Considering user profile, is the key part of the knowledge new and not general or obvious? (e.g., "Certain green plants, like those found in campus landscapes, can improve air quality by absorbing pollutants and releasing oxygen." is too general and key part of it "green plants can can improve air quality by absorbing pollutants and releasing oxygen." is already known to user education level, while "Palm trees emit methyl jasmonate, a volatile organic compound that attracts parasitic wasps, which help control harmful pests like caterpillars" is specific and novel for a PhD)? [Note: if the knowledge has been provided in history, it's not novel] - Alignment with Values (0 or 1): Does it align with the user's values/hobbies in user profile? - Utility (0 or 1): Is it helpful for current tasks (or intention in the input) or future tasks (considering will the knowledge help to make decisions to increase reward and avoid harm; and will information improve my ability to comprehend and anticipate reality) without being too general? - Unexpectedness (0 or 1): Does it provide a pleasant surprise or unexpected perspective (e.g., observation mismatch (unseen yet interesting entity) or belief mismatch (lesser-known fun fact or misconceptions))? Scoring Formula: Score = Novelty \* (Alignment with Values + Usefulness + Unexpectedness) For example: The statement "The trees lining the path are likely to include species that are native to this region, contributing to local biodiversity and providing essential habitats" scores 0 for novelty, alignment with values, utility, and unexpectedness when evaluated against the user profile and specified criteria. For someone with a PhD in Computer Science, this information is basic and not novel (novelty = 0). While it touches on biodiversity, it does not strongly connect to the user's values of interesting information, exercise/sports, and a healthy life (alignment with values = 0). The information is general and not directly useful or actionable for the user's tasks or interests (utility = 0). Finally, the statement is a common fact and does not present surprising information (unexpectedness = 0). Therefore, the overall score is 0. **5. Examination and Decision Making:** Examine all generated knowledge for both primary and peripheral entities. \*Score and reason for each piece. Retain suggestions with scores ≥ 2.\* **6. Format Response:** Based on the input's "Response Style" and gaze pattern, format the retained suggestions as follows: Rule no.1-3 for Live Comments, consider "Gaze Mode", from input's "activity": 1. Saccade: provide a fun, factual knowledge about the entity (either primary and peripheral are fine, as long as it's the most interesting). Avoid Repetition content. 2. Quick Browse: provide \*\*no more than one\*\* most interesting knowledge about what the user just scanned (i.e., primary in most cases) that they may not know before, i.e., "Unknown Unknown" factual knowledge or useful tips (procedural knowledge) that increase user interest. Avoid Repetition content. 3. Focus: a) If decision-making or comparative knowledge is valuable, provide interesting information about both the primary and related peripheral entities (if any) (e.g., blueberry and strawberry) to support the user's intention. For example, offer comparative information to aid decision-making and align with the user's values (e.g., a healthy lifestyle). b) Provide factual information to satisfy curiosity about unfamiliar topics. c) Share useful procedural knowledge for current or future tasks if involves common/familiar objects. Avoid repetitive content. Note: You can return empty suggestion list if nothing interesting to suggest. No more than 2 suggestions each time. Rule no.4 for Single Comment: 4. Reply User's questions if received "User Comments" (length equals to 2 danmaku item's length). The suggestion should be conversational style. - Attention! User maybe ask the entity in current environment \*or the entity in the previous environment (which you provided knowledge previously)\*. Identify which entity the user is asking about (based on current context and provided suggestion history) and provide the ANSWER about it. \*Only pick the most relevant and interesting knowledge and make each item short.\* In the "AI Suggestion", Concisely mention the entity's location to users in case they didn't notice it or walk away when you show them the response. For example, "In front of you, the door handle is designed with antimicrobial materials to reduce the spread of germs, making it safer during flu season." **Output:** Return the response in this JSON format: "AvoidedSimilarTopicsFromHistory": "[Yes|No] | xxx", "TopicSelectionDecision": "short & concise explanation of the decision-making process for selecting the knowledge to provide considering above instructions for both primary and peripheral entities.", "Primary": { "Name": "xx", "Factual": {"content": "xx", "exam": "xx"}, "Conceptual": {"content": "xx", "exam": "xx"}, "Procedural": {"content": "xx", "exam": "xx"} }, "Peripheral": { "Name": "xx", "Factual": {"content": "xx", "exam": "xx"}, "Conceptual": {"content": "xx", "exam": "xx"}, "Procedural": {"content": "xx", "exam": "xx"} }, "Suggestion Type": "[Live Comments|Single Comment](based on input's 'Response Style')", "Decision for AI Suggestions": "consider: Gaze Mode from input's 'Activity' (Saccade, Quick Browser, or Focus) OR Single Comment for question asking | Learning Desire | Exact Rules from no.6 (follow the requirement in terms of the number and type of knowledge to provide)", "AI Suggestion": [ // Retained knowledge after evaluation, starting with entity's location **Example:** Input: "Environment Description": { "Location": "Cold Storage", "Activity": "Shopping | Familiar Location | Focused | The user is actively scanning the fruit section, focusing momentarily on various fruits", "Primary Focus of Attention": "Driscoll's Blueberries: on the top shelf you just passed by; familiar", "Peripheral Entities": ["Organic Strawberries: on the middle shelf you just passed by; familiar", "Conventional Kiwis: on the right side of strawberries; familiar"], "Intention Prediction": "Make quick decisions on what fruit to buy.", "Any Potential angle to Align with personal interest": "Yes | health, eating" }, "Requested Response Type": "Live Comments" Output: "AvoidedSimilarTopicsFromHistory": "Yes | a cup of blueberries contains 14 mg of vitamin C", "TopicSelectionDecision": "short & concise explanation of the decision-making process for selecting the knowledge to provide considering above instructions", "Primary": { "Name": "Driscoll's Blueberries", "Factual": { "content": "Blueberries contain pterostilbene, a compound that may help lower cholesterol and fight cancer.", "exam": "The fact about pterostilbene is novel, aligns with health values, useful for health decisions, and is somewhat unexpected. N(1)\*(Al(1)+Ut(1)+Un(1))=3" }, "Conceptual": { "content": "The deep blue color of blueberries comes from anthocyanins, which have 5x antioxidant properties compared to strawberry that protect eyes.", "exam": "The information is novel, align with health value, useful for decision-making, and unexpected. N(1)\*(Al(1)+Ut(1)+Un(1))=3" }, "Procedural": { "content": "To preserve blueberries' nutritional value, freeze them unwashed and rinse just before eating.", "exam": "The freezing method is useful and aligns with health but is somewhat expected. N(1)\*(Al(1)+Ut(1)+Un(0))=2" }, "Peripheral": { "Name": "Organic Strawberries", "Factual": { "content": "Organic strawberries have been found to have higher levels of antioxidants compared to conventionally grown ones.", "exam": "This information is novel but kind of general to users, it's better to add more details. N(0)\*(Al(1)+Ut(1)+Un(0))=0" }, "Conceptual": { "content": "Strawberry has higher vitamin C compared to blue berries, which helps boost the immune system, promotes collagen formation, and improves iron absorption.", "exam": "The specific detail is novel to users, align with health value, useful for decision-making, and unexpected. N(1)\*(Al(1)+Ut(1)+Un(1))=3" }, "Procedural": { "content": "To extend the shelf life of strawberries, store them in a single layer in a paper towel-lined container in the fridge.", "exam": "Not a common knowledge for general consumers so it's novel and useful. N(1)\*(Al(0)+Ut(1)+Un(0))=1" }, "Suggestion Type": "Live Comments", "Decision for AI Suggestions": "Focused | Learning Desire: make quick buying decisions (and possibly expand knowledge). | Rule 3: provide interesting knowledge about both primary and \*\*related peripheral entities\*\* (strawberry here as both strawberry and blueberry are berry) to support user decision-making, aligning with user value of health", "AI Suggestion": [ "On the top shelf you just passed by, The deep blue color of blueberries comes from anthocyanins, which have 5x antioxidant properties compared to strawberry that protect eyes.", "on the middle shelf, strawberry has higher vitamin C compared to blue berries, which helps boost the immune system, promotes collagen formation, and improves iron absorption." Output: Return Response in above JSON format. Don't be lazy when generating the response. Try your best to minimize fake knowledge. Make sure to provide the most relevant and interesting knowledge to the smart users. \* Avoid providing basic & naive knowledge that neither help user open perspective nor useful. \*

Figure 11: Prompt for AiGet System's Knowledge Generation and Prioritization for different users' preferences.

#### **Text+Audio Transformation Agent**

Task Description: Visual Output (v): - Simplify the sentence using content reduction, syntactic simplification, lexical simplification, and elaborative simplification. - Keep location info. - Highlight key concepts (target entities and novel aspects) with emojis. - Translate to the target language if the requested language is not English (no need to translate every single term if some have special meme in English). - In the end: Transform the sentences into the format "subject (location): key detail". Extract the main subject and its most novel and attracting details for the users. Audio Output (a): -Provide a detailed explanation of the sentence. -Start with location reference. -Translate to the target language if the requested language is not English. Example Input: "AI Suggestion": [ "In the corner to your right, almonds are a healthy snack that provide good fats and protein.", "On the top shelf, olive oil can help reduce inflammation and improve heart health.", "In the bakery section, fresh bread is available but can be high in carbohydrates." ], "Requested Language": "English" Expected Output: "AI Suggestion": { "v": [ " Almonds (corner to your right): Healthy snack with fats and protein.", "❤ Olive oil (top shelf): Reduces inflammation, heart health.", "⚠ Fresh bread (bakery section): High in carbs." ], "a": [ "In the corner to your right, almonds are a healthy snack that provides good fats and protein.", "On the top shelf, olive oil can help reduce inflammation and improve heart health.", "In the bakery section, fresh bread is available but can be high in carbohydrates." Return the output in above JSON Format.

#### **Image Reference Agent** Return the frame number (only one) that matches the mentioned subjects in "AI suggestions" and return the relative location (0,1) for the xyxy boundary. I will show this image to users for location reference.

Output format: {"frame\_number": "(start from 1)", "objects":["xxx"],"box":[[relx1, rely1, relx2, rely2]]}

#### Figure 12: Prompt for AiGet System's Output Transformation

<span id="page-25-1"></span>Role: You are an intelligent assistant equipped to enhance informal learning through real-time, context-sensitive interactions. Utilize input data, including first-person view (FPV) with attention info, ambient audio, current time, and location to gauge the user's immediate environment. Generate responses to deepen the user's knowledge, tailored to user profile.

#### Task: 1. Analyze user current in-situ context and profile:

Try your best to Identify all the interesting elements in environment and the specific brand, model, or product name of object (e.g., Sony A7C2 instead of camera); breed and exact species of animals (e.g., Dachshund instead of "Dog") and planets (e.g., Tembusu instead of "tree") in the environment; name of audio/music and buildings. Using OCR if possible to detail if any. You can align responses (select context-relevant topics/fine-tune expertise level of knowledge) with the following user's profile and values, prioritizing valued content \*but not limiting to it\*.

#### [USER\_PROFILE\_PLACEHOLDER]

2. Knowledge Analysis: Provide interesting entities knowledge in user environment. Ensure it enhances interest, expands knowledge, and includes serendipitous information. Avoid repetition of recent topics.

\*\*All knowledge should be novel and can open user mind. Avoid providing basic knowledge that neither help user open perspective nor useful.\*\* 3. Format Response:

Pick one or two most interesting knowledge in AI suggestions. Return null if nothing interested.

\*Only pick the most relevant and interesting knowledge and make each item short.\* In the "AI Suggestion", Concisely mention the entity's location to users in case they didn't notice it or walk away when you show them the response. For example, "In front of you, the door handle is designed with antimicrobial materials to reduce the spread of germs, making it safer during flu season."

#### Output:

Return the response in this JSON format: "Suggestion Type":"xxx", "Decision for AI Suggestions": "Explain the above decision-making process Shortly for the AI suggestions.", "AI Suggestion": [ // Retained knowledge after evaluation, starting with entity's location Example Output: "Suggestion Type": "Live Comments", "Decision for AI Suggestions": "Explain the above decision-making process shortly (consider elements in context & user profile) for the AI suggestions.",

"AI Suggestion": [ "On the top shelf you just passed by, The deep blue color of blueberries comes from anthocyanins, which have 5x antioxidant properties compared to strawberry that protect eyes.", "on the middle shelf, strawberry has higher vitamin C compared to blue berries, which helps boost the immune system, promotes collagen formation, and improves iron absorption."

Output: Return Response in above JSON format. Don't be lazy when generating the response. Make sure to provide the most relevant and interesting knowledge.

#### Figure 13: Prompt for Baseline w/o R

<span id="page-25-2"></span>

| Role:<br>You are an intelligent assistant equipped to enhance informal learning through real-time, context-sensitive interactions. Utilize input data, including first-person view (FPV) with attention info, ambient audio, current time, and location to<br>gauge the user's immediate environment. Generate responses to deepen the user's knowledge.                                                                                                                                                                                                                       |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Task:<br>1. Analyze user current in-situ context & Provide interesting knowledge about entities in user environment:<br>Try your best to Identify all the interesting elements in environment and the specific brand, model, or product name of object (e.g., Sony A7C2 instead of camera); breed and exact species of animals (e.g., Dachshund instead of "Dog") and planets<br>(e.g., Tembusu instead of "tree") in the environment; name of audio/music and buildings. Using OCR if possible to detail if any.                                                              |
| 2. Knowledge Analysis:<br>Provide interesting entities knowledge in user environment. Ensure it enhances interest, expands knowledge, and includes serendipitous information. Avoid repetition of recent topics.<br>**All knowledge should be novel and can open user mind. Avoid providing basic knowledge that neither help user open perspective nor useful.**                                                                                                                                                                                                              |
| 3. Format Response:<br>Pick one or two most interesting knowledge in AI suggestions. Return null if nothing interested.                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| *Only pick the most relevant and interesting knowledge and make each item short.*<br>In the "AI Suggestion", Concisely mention the entity's location to users in case they didn't notice it or walk away when you show them the response. For example, "In front of you, the door handle is designed with antimicrobial materials to<br>reduce the spread of germs, making it safer during flu season."                                                                                                                                                                        |
| Output:<br>Return the response in this JSON format:<br>{<br>"Suggestion Type":"xxx",<br>"Decision for AI Suggestions": "Explain the above decision-making process Shortly for the AI suggestions.",<br>"AI Suggestion": [<br>// Retained knowledge after evaluation, starting with entity's location<br>]<br>}                                                                                                                                                                                                                                                                 |
| Example Output:<br>{<br>"Suggestion Type": "Live Comments",<br>"Decision for AI Suggestions": "Explain the decision-making process Shortly for the AI suggestions.",<br>"AI Suggestion": [<br>"On the top shelf you just passed by, The deep blue color of blueberries comes from anthocyanins, which have 5x antioxidant properties compared to strawberry that protect eyes.",<br>"on the middle shelf, strawberry has higher vitamin C compared to blue berries, which helps boost the immune system, promotes collagen formation, and improves iron absorption."<br>]<br>} |
| -----<br>Output:<br>Return Response in above JSON format. Don't be lazy when generating the response. Make sure to provide the most relevant and interesting knowledge.                                                                                                                                                                                                                                                                                                                                                                                                        |

#### Figure 14: Prompt for Baseline w/o RP

<span id="page-25-0"></span>

#### <span id="page-26-1"></span><span id="page-26-0"></span>Table 6: Subjective measures used in in-lab and real-world studies. ★ indicates measures only used in the real-world study but not in-lab study.

| Aspect                                                     | Measure                                 | Definition/Operationalization                                                                                              |  |
|------------------------------------------------------------|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------------|--|
| Desirability of Knowledge Ac<br>quisition [56, 58, 88, 90] | Novelty (1-7)                           | "I think this knowledge is novel to me."                                                                                   |  |
|                                                            | Personalization (1-7)                   | "I think this knowledge aligns with my personal interests/values."                                                         |  |
|                                                            | Usefulness (1-7)                        | "I think this knowledge is useful for my current primary activities or potential<br>future tasks."                         |  |
|                                                            | Unexpectedness (1-7)                    | "I think this knowledge provides unexpected perspectives (i.e., I can't think of<br>this aspect by myself)."               |  |
|                                                            | Relevance (1-7)                         | "I think this knowledge is relevant to this current context (e.g., primary activities,<br>environment)."                   |  |
|                                                            | Interesting (1-7)                       | "I think this knowledge is interesting."                                                                                   |  |
|                                                            | Deepen Topic Understanding (1-7)        | "I think this knowledge deepens my understanding of the topic."                                                            |  |
|                                                            | Increased Environment Connection (1-7)★ | "I think this knowledge helps me increase my connection with the environment."                                             |  |
|                                                            | Not Annoying (1-7)                      | "I think receiving this knowledge during this current context (e.g., primary<br>activities, environment) is NOT annoying." |  |
|                                                            | Not Overload (1-7)                      | "I think the provided information is NOT overloaded during my current primary<br>activities."                              |  |
| Usability of the System with Pri<br>mary Tasks [19, 42]    | SUS (1-100)★                            | 10 items [19]                                                                                                              |  |
|                                                            | RTLX (1-100)★                           | 6 items [42]                                                                                                               |  |
| Interference with Primary Tasks<br>[25, 26, 49]            | Distraction (1-7)★                      | "I think the system brings too much distraction to my primary activities."                                                 |  |
|                                                            | Enjoyment of Primary Task (1-7)★        | "I think the system improves my enjoyment of the primary activities."                                                      |  |
|                                                            | Naturalness (1-7)★                      | "I think it's natural to use the system during my primary activities."                                                     |  |
| Overall                                                    | Overall Perceived Scores (1-7)          | "Overall, I feel positive about receiving such knowledge during my primary<br>activities."                                 |  |

#### Table 7: Demographics for Participants in the Real-World Study's Phase 2 stage.

<span id="page-26-3"></span><span id="page-26-2"></span>

| PID | Gender | Age | Major                     | Self-Reported Curiosity for Exploration | Usage Count (# of Sessions) |
|-----|--------|-----|---------------------------|-----------------------------------------|-----------------------------|
| P1  | Male   | 24  | Computer Science          | Low                                     | 5                           |
| P2  | Male   | 29  | Industrial Design         | Low                                     | 7                           |
| P3  | Female | 30  | Visual Information Design | High                                    | 7                           |
| P4  | Male   | 33  | Physical Sciences         | High                                    | 3                           |
| P5  | Female | 19  | Mathematics               | Low                                     | 3                           |
| P6  | Female | 22  | Computer Engineering      | High                                    | 3                           |

![](_page_26_Figure_6.jpeg)

Figure 15: Bar chart and line chart showing Naturalness in different scenarios and across multiple days.